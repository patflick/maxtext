{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('run_name', None),\n",
       "             ('load_parameters_path', ''),\n",
       "             ('load_from_other_directory', ''),\n",
       "             ('load_from_other_directory_step', -1),\n",
       "             ('reuse_example_batch', 0),\n",
       "             ('metrics_file', ''),\n",
       "             ('gcs_metrics', False),\n",
       "             ('dtype', dtype(bfloat16)),\n",
       "             ('int8_training', False),\n",
       "             ('use_dqdg', False),\n",
       "             ('fwd_int8', True),\n",
       "             ('dlhs_int8', True),\n",
       "             ('drhs_int8', False),\n",
       "             ('fwd_int8_qk', False),\n",
       "             ('dlhs_int8_qk', False),\n",
       "             ('drhs_int8_qk', False),\n",
       "             ('fwd_int8_pv', True),\n",
       "             ('dlhs_int8_pv', True),\n",
       "             ('drhs_int8_pv', False),\n",
       "             ('aqt_use_dummy_static_bound', False),\n",
       "             ('aqt_use_fwd_quant', False),\n",
       "             ('aqt_rng_type', 'jax.uniform'),\n",
       "             ('global_parameter_scale', 1),\n",
       "             ('base_emb_dim', 2560),\n",
       "             ('base_num_heads', 8),\n",
       "             ('base_mlp_dim', 8192),\n",
       "             ('base_num_decoder_layers', 16),\n",
       "             ('head_dim', 256),\n",
       "             ('mlp_activations', ['relu']),\n",
       "             ('dropout_rate', 0),\n",
       "             ('logits_via_embedding', True),\n",
       "             ('remat_policy', 'full'),\n",
       "             ('scan_layers', True),\n",
       "             ('param_scan_axis', 1),\n",
       "             ('enable_flash_attention', True),\n",
       "             ('record_internal_nn_metrics', 0),\n",
       "             ('base_output_directory', ''),\n",
       "             ('mesh_axes', ['data', 'fsdp', 'tensor']),\n",
       "             ('logical_axis_rules',\n",
       "              (('activation_batch', ('data', 'fsdp')),\n",
       "               ('activation_length', ('data', 'fsdp')),\n",
       "               ('activation_embed', 'tensor'),\n",
       "               ('activation_mlp', 'tensor'),\n",
       "               ('activation_heads', 'tensor'),\n",
       "               ('activation_kv', 'tensor'),\n",
       "               ('activation_vocab', 'tensor'),\n",
       "               ('mlp', 'tensor'),\n",
       "               ('vocab', 'tensor'),\n",
       "               ('embed', 'fsdp'),\n",
       "               ('heads', 'tensor'))),\n",
       "             ('data_sharding', (('data', 'fsdp', 'tensor'),)),\n",
       "             ('dcn_data_parallelism', -1),\n",
       "             ('dcn_fsdp_parallelism', 1),\n",
       "             ('dcn_tensor_parallelism', 4),\n",
       "             ('ici_data_parallelism', 1),\n",
       "             ('ici_fsdp_parallelism', -1),\n",
       "             ('ici_tensor_parallelism', 4),\n",
       "             ('dataset_path', ''),\n",
       "             ('vocab_size', 32768),\n",
       "             ('assets_path', 'assets'),\n",
       "             ('vocab_relative_path', 'tokenizer'),\n",
       "             ('dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_split', 'validation'),\n",
       "             ('per_device_batch_size', 12.0),\n",
       "             ('eval_per_device_batch_size', 0),\n",
       "             ('max_corpus_chars', 10000000),\n",
       "             ('dataset_type', 'c4'),\n",
       "             ('file_pattern_for_train_data', ''),\n",
       "             ('file_pattern_for_eval_data', ''),\n",
       "             ('steps', 150001),\n",
       "             ('log_period', 100),\n",
       "             ('save_period', 3399),\n",
       "             ('learning_rate', 3e-05),\n",
       "             ('cosine_learning_rate_final_fraction', 0.1),\n",
       "             ('warmup_steps_fraction', 0.1),\n",
       "             ('learning_rate_schedule_steps', 150001),\n",
       "             ('max_target_length', 2048),\n",
       "             ('max_eval_target_length', 512),\n",
       "             ('max_predict_length', 64),\n",
       "             ('sampling_temperature', 0.6),\n",
       "             ('sampling_top_k', 20),\n",
       "             ('eos_id', 2),\n",
       "             ('prompt', 'I love to '),\n",
       "             ('enable_profiler', False),\n",
       "             ('enable_checkpointing', True),\n",
       "             ('async_checkpointing', True),\n",
       "             ('enable_dropout', True),\n",
       "             ('enable_data_shuffling', True),\n",
       "             ('data_shuffle_seed', 0),\n",
       "             ('init_weights_seed', 0),\n",
       "             ('gradient_clipping_threshold', 1.0),\n",
       "             ('adam_b1', 0.9),\n",
       "             ('adam_b2', 0.95),\n",
       "             ('adam_eps', 1e-08),\n",
       "             ('adam_eps_root', 0.0),\n",
       "             ('adam_weight_decay', 0.1),\n",
       "             ('collect_stack_trace', False),\n",
       "             ('stack_trace_to_cloud', False),\n",
       "             ('stack_trace_interval_seconds', 600),\n",
       "             ('use_iota_embed', False),\n",
       "             ('compiled_trainstep_file', ''),\n",
       "             ('compile_topology', ''),\n",
       "             ('compile_topology_num_slices', -1),\n",
       "             ('emb_dim', 2560),\n",
       "             ('num_heads', 8),\n",
       "             ('mlp_dim', 8192),\n",
       "             ('num_decoder_layers', 16),\n",
       "             ('global_batch_size_to_load', 48),\n",
       "             ('global_batch_size_to_train_on', 48)])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up mesh config for v4-8\n",
    "extra_config = {\n",
    "    \"dcn_tensor_parallelism\": 4,\n",
    "    \"ici_tensor_parallelism\": 4,\n",
    "}\n",
    "\n",
    "import pyconfig\n",
    "\n",
    "def get_config(config_file, extra_config):\n",
    "    args = [\"\", config_file]\n",
    "    for k, v in extra_config.items():\n",
    "        args.append(f\"{k}={v}\")\n",
    "    pyconfig.initialize(args)\n",
    "\n",
    "    return pyconfig.config\n",
    "\n",
    "config = get_config(\"configs/base-1b.yml\", extra_config)\n",
    "pyconfig._config.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] (num_devices: 4)\n",
      "Decided on mesh: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)\n",
      "   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]]]\n"
     ]
    }
   ],
   "source": [
    "# explicitly create mesh\n",
    "import max_utils  \n",
    "from jax.sharding import Mesh\n",
    "from input_pipeline import create_data_iterator_with_tokenizer\n",
    "\n",
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = Mesh(devices_array, config.mesh_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24ee6991b0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24ee6991b0>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24ee6991b0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24ee6991b0>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24ee6991b0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24ee6991b0>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f394ece3640> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f394ece3640>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f394ece3640> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f394ece3640>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f394ece3640> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f394ece3640>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24c152e710> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24c152e710>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24c152e710> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24c152e710>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24c152e710> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _GrainMapFn.__call__.<locals>.<lambda> at 0x7f24c152e710>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "# coding=utf-8\n",
      "lambda k, s: k[:-len(s)] if k.endswith(s) else k\n",
      "# coding=utf-8\n",
      "lambda x: {k: x[k][:l, ...] for (k, l) in feature_lengths.items()}\n",
      "# coding=utf-8\n",
      "lambda x: self._map_fn_with_special_kwargs(x, *args, **kwargs)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function trim_dataset.<locals>.<lambda> at 0x7f394ece3d90> and will run it as-is.\n",
      "Cause: could not parse the source code of <function trim_dataset.<locals>.<lambda> at 0x7f394ece3d90>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trim_dataset.<locals>.<lambda> at 0x7f394ece3d90> and will run it as-is.\n",
      "Cause: could not parse the source code of <function trim_dataset.<locals>.<lambda> at 0x7f394ece3d90>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trim_dataset.<locals>.<lambda> at 0x7f394ece3d90> and will run it as-is.\n",
      "Cause: could not parse the source code of <function trim_dataset.<locals>.<lambda> at 0x7f394ece3d90>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "# coding=utf-8\n",
      "lambda ex: {k: _trim(k, v) for (k, v) in ex.items()}\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function trim_and_pad_dataset.<locals>.<lambda> at 0x7f394ece29e0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function trim_and_pad_dataset.<locals>.<lambda> at 0x7f394ece29e0>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trim_and_pad_dataset.<locals>.<lambda> at 0x7f394ece29e0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function trim_and_pad_dataset.<locals>.<lambda> at 0x7f394ece29e0>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trim_and_pad_dataset.<locals>.<lambda> at 0x7f394ece29e0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function trim_and_pad_dataset.<locals>.<lambda> at 0x7f394ece29e0>: no matching AST found among candidates:\n",
      "# coding=utf-8\n",
      "lambda x: {k: _trim_and_pad(k, t) for (k, t) in x.items()}\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/patflick/.local/lib/python3.10/site-packages/seqio/feature_converters.py\", line 1051, in _convert_example  *\n        lm_features = super()._convert_example(features)\n    File \"/home/patflick/.local/lib/python3.10/site-packages/seqio/feature_converters.py\", line 853, in _convert_example  *\n        decoder_input_tokens = utils.make_autoregressive_inputs(\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf___shift_right_by_one() got an unexpected keyword argument 'sequence_id'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/patflick/dev/git/maxtext/MaxText/repro_cnndm_problem.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/repro_cnndm_problem.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcnndm_data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/repro_cnndm_problem.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# FIXME: this currently fails!\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/repro_cnndm_problem.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m train_ds, eval_ds \u001b[39m=\u001b[39m cnndm_data\u001b[39m.\u001b[39;49mmake_cnndm_train_iterator_and_tokenizer(config)\n",
      "File \u001b[0;32m~/dev/git/maxtext/MaxText/cnndm_data.py:187\u001b[0m, in \u001b[0;36mmake_cnndm_train_iterator_and_tokenizer\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_cnndm_train_iterator_and_tokenizer\u001b[39m(config):\n\u001b[1;32m    186\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\" Make train iterator and tokenizer for cnndm dataset\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m   train_ds, eval_ds \u001b[39m=\u001b[39m get_cnndm_datasets(\n\u001b[1;32m    188\u001b[0m     percore_batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m    189\u001b[0m   )\n\u001b[1;32m    191\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39mfilter_keys\u001b[39m(record):\n\u001b[1;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m: record[\u001b[39m'\u001b[39m\u001b[39mdecoder_input_tokens\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m: record[\u001b[39m'\u001b[39m\u001b[39mdecoder_target_tokens\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    193\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minputs_segmentation\u001b[39m\u001b[39m'\u001b[39m:record[\u001b[39m'\u001b[39m\u001b[39mdecoder_causal_attention\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m    194\u001b[0m     }\n",
      "File \u001b[0;32m~/dev/git/maxtext/MaxText/cnndm_data.py:176\u001b[0m, in \u001b[0;36mget_cnndm_datasets\u001b[0;34m(percore_batch_size)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cnndm_datasets\u001b[39m(\n\u001b[1;32m    174\u001b[0m   percore_batch_size\n\u001b[1;32m    175\u001b[0m ):\n\u001b[0;32m--> 176\u001b[0m   train_ds \u001b[39m=\u001b[39m _dataset_common(is_training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, percore_batch_size\u001b[39m=\u001b[39;49mpercore_batch_size, TRAINING_SEED\u001b[39m=\u001b[39;49m\u001b[39m1234\u001b[39;49m)\n\u001b[1;32m    177\u001b[0m   \u001b[39m# shard the dataset as soon as it is loaded\u001b[39;00m\n\u001b[1;32m    178\u001b[0m   train_ds \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39mshard(num_shards \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mprocess_count(), index \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mprocess_index())\n",
      "File \u001b[0;32m~/dev/git/maxtext/MaxText/cnndm_data.py:155\u001b[0m, in \u001b[0;36m_dataset_common\u001b[0;34m(is_training, percore_batch_size, TRAINING_SEED, MAX_SEQ_LEN, OUTPUTS_LENGTH, TRAINING_NUM_BATCHES_TO_SKIP, shard_info)\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[39m# TODO(sgpyc): enable sync of seeds across hosts, currently the\u001b[39;00m\n\u001b[1;32m    150\u001b[0m   \u001b[39m# following failed because of \"sync_global_devices name mismatch\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m   \u001b[39m# seed = jnp.int32(multihost_utils.broadcast_one_to_all(seed))\u001b[39;00m\n\u001b[1;32m    152\u001b[0m   logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mTrain input seed: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    153\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m seed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m seed)\n\u001b[0;32m--> 155\u001b[0m dataset \u001b[39m=\u001b[39m seqio\u001b[39m.\u001b[39;49mget_dataset(\n\u001b[1;32m    156\u001b[0m   mixture_or_task_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcnn_dailymail_v001_3.4.0\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m is_training \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mcnn_dailymail_v001_eval_3.4.0\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m   task_feature_lengths\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    158\u001b[0m   \u001b[39m'\u001b[39;49m\u001b[39minputs\u001b[39;49m\u001b[39m'\u001b[39;49m: MAX_SEQ_LEN,\n\u001b[1;32m    159\u001b[0m   \u001b[39m'\u001b[39;49m\u001b[39mtargets\u001b[39;49m\u001b[39m'\u001b[39;49m: OUTPUTS_LENGTH},         \n\u001b[1;32m    160\u001b[0m   dataset_split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m is_training \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    161\u001b[0m   use_cached\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    162\u001b[0m   num_epochs\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    163\u001b[0m   feature_converter\u001b[39m=\u001b[39;49mseqio\u001b[39m.\u001b[39;49mseqio\u001b[39m.\u001b[39;49mDecoderFeatureConverter(\n\u001b[1;32m    164\u001b[0m       pack\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    165\u001b[0m   ),\n\u001b[1;32m    166\u001b[0m   shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m is_training \u001b[39melse\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    167\u001b[0m   batch_size\u001b[39m=\u001b[39;49mbatch_size_per_process,\n\u001b[1;32m    168\u001b[0m   shard_info \u001b[39m=\u001b[39;49m shard_info,\n\u001b[1;32m    169\u001b[0m   seed \u001b[39m=\u001b[39;49m seed,\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/dataset_providers.py:2417\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(mixture_or_task_name, task_feature_lengths, feature_converter, dataset_split, use_cached, shuffle, num_epochs, shard_info, verbose, seed, batch_size, trim_output_features)\u001b[0m\n\u001b[1;32m   2404\u001b[0m   ds \u001b[39m=\u001b[39m mixture_or_task\u001b[39m.\u001b[39mget_dataset(\n\u001b[1;32m   2405\u001b[0m       sequence_length\u001b[39m=\u001b[39mtask_feature_lengths,\n\u001b[1;32m   2406\u001b[0m       split\u001b[39m=\u001b[39mdataset_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2414\u001b[0m       trim_output_features\u001b[39m=\u001b[39mtrim_output_features,\n\u001b[1;32m   2415\u001b[0m   )\n\u001b[1;32m   2416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2417\u001b[0m   ds \u001b[39m=\u001b[39m mixture_or_task\u001b[39m.\u001b[39mget_dataset(\n\u001b[1;32m   2418\u001b[0m       task_feature_lengths,\n\u001b[1;32m   2419\u001b[0m       split\u001b[39m=\u001b[39mdataset_split,\n\u001b[1;32m   2420\u001b[0m       use_cached\u001b[39m=\u001b[39muse_cached,\n\u001b[1;32m   2421\u001b[0m       shuffle\u001b[39m=\u001b[39mshuffle,\n\u001b[1;32m   2422\u001b[0m       seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m   2423\u001b[0m       shard_info\u001b[39m=\u001b[39mshard_info,\n\u001b[1;32m   2424\u001b[0m       num_epochs\u001b[39m=\u001b[39mnum_epochs,\n\u001b[1;32m   2425\u001b[0m       trim_output_features\u001b[39m=\u001b[39mtrim_output_features,\n\u001b[1;32m   2426\u001b[0m   )\n\u001b[1;32m   2427\u001b[0m   ds \u001b[39m=\u001b[39m feature_converter(ds, task_feature_lengths\u001b[39m=\u001b[39mtask_feature_lengths)\n\u001b[1;32m   2428\u001b[0m   \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/feature_converters.py:1405\u001b[0m, in \u001b[0;36mDecoderFeatureConverter.__call__\u001b[0;34m(self, ds, task_feature_lengths)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefixsuffixlm_feature_converter(ds, task_feature_lengths)\n\u001b[1;32m   1404\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m task_feature_lengths:\n\u001b[0;32m-> 1405\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprefixlm_feature_converter(ds, task_feature_lengths)\n\u001b[1;32m   1406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrictlm_feature_converter(ds, task_feature_lengths)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/feature_converters.py:484\u001b[0m, in \u001b[0;36mFeatureConverter.__call__\u001b[0;34m(self, ds, task_feature_lengths)\u001b[0m\n\u001b[1;32m    474\u001b[0m ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dataset(\n\u001b[1;32m    475\u001b[0m     ds,\n\u001b[1;32m    476\u001b[0m     expected_features\u001b[39m=\u001b[39mtask_features_with_passthrough,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m     error_label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    481\u001b[0m )\n\u001b[1;32m    483\u001b[0m \u001b[39m# Conversion 1: implemented by subclass\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_features(ds, task_feature_lengths)\n\u001b[1;32m    486\u001b[0m expected_features \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMODEL_FEATURES)\n\u001b[1;32m    487\u001b[0m expected_features\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_passthrough_features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/feature_converters.py:1177\u001b[0m, in \u001b[0;36mPrefixLMFeatureConverter._convert_features\u001b[0;34m(self, ds, task_feature_lengths)\u001b[0m\n\u001b[1;32m   1174\u001b[0m ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pack_or_pad(ds, concat_task_feature_lengths)\n\u001b[1;32m   1175\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mmap(restore_0s, num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m   1178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_example, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mexperimental\u001b[39m.\u001b[39;49mAUTOTUNE\n\u001b[1;32m   1179\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2278\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2275\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2278\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39;49m_map_v2(\n\u001b[1;32m   2279\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2280\u001b[0m     map_func,\n\u001b[1;32m   2281\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[1;32m   2282\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m   2283\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[1;32m     43\u001b[0m       num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[1;32m     44\u001b[0m       deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:148\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m--> 148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m    149\u001b[0m     map_func,\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[1;32m    151\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[1;32m    152\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:272\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    273\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1189\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1188\u001b[0m   \u001b[39m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1190\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1169\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 1169\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m   1173\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 694\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    695\u001b[0m     \u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    696\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    699\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:176\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 176\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    169\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:398\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[1;32m    396\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m--> 398\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[1;32m    399\u001b[0m     args, kwargs, func_graph)\n\u001b[1;32m    401\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:305\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[1;32m    304\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 305\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    307\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    308\u001b[0m         args,\n\u001b[1;32m    309\u001b[0m         kwargs,\n\u001b[1;32m    310\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    311\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    312\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    313\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[1;32m    314\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    316\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    317\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1052\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m   1054\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1055\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1057\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    594\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 597\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    598\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:238\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    239\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    240\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:168\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    167\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 168\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[1;32m    169\u001b[0m ret \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:693\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 693\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m    694\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileyz_lyxj3.py:64\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___convert_example\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     62\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     63\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 64\u001b[0m lm_features \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39msuper\u001b[39;49m), (), \u001b[39mNone\u001b[39;49;00m, fscope)\u001b[39m.\u001b[39;49m_convert_example, (ag__\u001b[39m.\u001b[39;49mld(features),), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     65\u001b[0m d \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mdict\u001b[39m), (ag__\u001b[39m.\u001b[39mld(lm_features),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:441\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args)\n\u001b[1;32m    442\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m   _attach_error_metadata(e, converted_f)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filetfqq5pd0.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___convert_example\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m      9\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 11\u001b[0m decoder_input_tokens \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(utils)\u001b[39m.\u001b[39;49mmake_autoregressive_inputs, (ag__\u001b[39m.\u001b[39;49mld(features)[\u001b[39m'\u001b[39;49m\u001b[39mtargets\u001b[39;49m\u001b[39m'\u001b[39;49m],), \u001b[39mdict\u001b[39;49m(sequence_id\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(features)\u001b[39m.\u001b[39;49mget, (\u001b[39m'\u001b[39;49m\u001b[39mtargets_segment_ids\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), \u001b[39mNone\u001b[39;49;00m, fscope), bos_id\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mbos_id), fscope)\n\u001b[1;32m     12\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdecoder_target_tokens\u001b[39m\u001b[39m'\u001b[39m: ag__\u001b[39m.\u001b[39mld(features)[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mdecoder_input_tokens\u001b[39m\u001b[39m'\u001b[39m: ag__\u001b[39m.\u001b[39mld(decoder_input_tokens), \u001b[39m'\u001b[39m\u001b[39mdecoder_loss_weights\u001b[39m\u001b[39m'\u001b[39m: ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(non_padding_position), (ag__\u001b[39m.\u001b[39mld(features)[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m],), \u001b[39mNone\u001b[39;00m, fscope)}\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/patflick/.local/lib/python3.10/site-packages/seqio/feature_converters.py\", line 1051, in _convert_example  *\n        lm_features = super()._convert_example(features)\n    File \"/home/patflick/.local/lib/python3.10/site-packages/seqio/feature_converters.py\", line 853, in _convert_example  *\n        decoder_input_tokens = utils.make_autoregressive_inputs(\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf___shift_right_by_one() got an unexpected keyword argument 'sequence_id'\n"
     ]
    }
   ],
   "source": [
    "# attempt to load cnn_dailymail dataset using qinwen's loader\n",
    "\n",
    "import cnndm_data\n",
    "\n",
    "\n",
    "# FIXME: this currently fails!\n",
    "train_ds, eval_ds = cnndm_data.make_cnndm_train_iterator_and_tokenizer(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
