{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1v-n-d84a2c81-w-0\n",
      "/home/patflick/dev/git/maxtext/MaxText\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('run_name', 'prompt-2b-v1'),\n",
       "             ('load_parameters_path', ''),\n",
       "             ('load_from_other_directory',\n",
       "              'gs://test-example-123/maxtest-lg/2b-c4/v4-32/maxtext_cnndm/checkpoints'),\n",
       "             ('load_from_other_directory_step', -1),\n",
       "             ('reuse_example_batch', 0),\n",
       "             ('metrics_file', ''),\n",
       "             ('gcs_metrics', False),\n",
       "             ('dtype', dtype(bfloat16)),\n",
       "             ('int8_training', False),\n",
       "             ('use_dqdg', False),\n",
       "             ('fwd_int8', True),\n",
       "             ('dlhs_int8', True),\n",
       "             ('drhs_int8', False),\n",
       "             ('fwd_int8_qk', False),\n",
       "             ('dlhs_int8_qk', False),\n",
       "             ('drhs_int8_qk', False),\n",
       "             ('fwd_int8_pv', True),\n",
       "             ('dlhs_int8_pv', True),\n",
       "             ('drhs_int8_pv', False),\n",
       "             ('aqt_use_dummy_static_bound', False),\n",
       "             ('aqt_use_fwd_quant', False),\n",
       "             ('aqt_rng_type', 'jax.uniform'),\n",
       "             ('global_parameter_scale', 1),\n",
       "             ('base_emb_dim', 2560),\n",
       "             ('base_num_heads', 8),\n",
       "             ('base_mlp_dim', 8192),\n",
       "             ('base_num_decoder_layers', 32),\n",
       "             ('head_dim', 256),\n",
       "             ('mlp_activations', ['relu']),\n",
       "             ('dropout_rate', 0),\n",
       "             ('logits_via_embedding', True),\n",
       "             ('remat_policy', 'full'),\n",
       "             ('scan_layers', True),\n",
       "             ('param_scan_axis', 1),\n",
       "             ('enable_flash_attention', True),\n",
       "             ('record_internal_nn_metrics', 0),\n",
       "             ('base_output_directory',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/2b/20231121'),\n",
       "             ('mesh_axes', ['data', 'fsdp', 'tensor']),\n",
       "             ('logical_axis_rules',\n",
       "              (('activation_batch', ('data', 'fsdp')),\n",
       "               ('activation_length', ('data', 'fsdp')),\n",
       "               ('activation_embed', 'tensor'),\n",
       "               ('activation_mlp', 'tensor'),\n",
       "               ('activation_heads', 'tensor'),\n",
       "               ('activation_kv', 'tensor'),\n",
       "               ('activation_vocab', 'tensor'),\n",
       "               ('mlp', 'tensor'),\n",
       "               ('vocab', 'tensor'),\n",
       "               ('embed', 'fsdp'),\n",
       "               ('heads', 'tensor'))),\n",
       "             ('data_sharding', (('data', 'fsdp', 'tensor'),)),\n",
       "             ('dcn_data_parallelism', -1),\n",
       "             ('dcn_fsdp_parallelism', 1),\n",
       "             ('dcn_tensor_parallelism', 4),\n",
       "             ('ici_data_parallelism', 1),\n",
       "             ('ici_fsdp_parallelism', -1),\n",
       "             ('ici_tensor_parallelism', 4),\n",
       "             ('dataset_path', 'gs://patflick-maxtext-lingvo/'),\n",
       "             ('vocab_size', 32768),\n",
       "             ('assets_path', '/home/patflick/dev/git/maxtext/assets'),\n",
       "             ('vocab_relative_path', 'tokenizer'),\n",
       "             ('dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_split', 'validation'),\n",
       "             ('per_device_batch_size', 12.0),\n",
       "             ('eval_per_device_batch_size', 0),\n",
       "             ('max_corpus_chars', 10000000),\n",
       "             ('dataset_type', 'c4'),\n",
       "             ('file_pattern_for_train_data', 'gs://test-example-123/datasets'),\n",
       "             ('steps', 10),\n",
       "             ('log_period', 100),\n",
       "             ('save_period', 4000),\n",
       "             ('learning_rate', 3e-05),\n",
       "             ('cosine_learning_rate_final_fraction', 0.1),\n",
       "             ('warmup_steps_fraction', 0.1),\n",
       "             ('learning_rate_schedule_steps', 10),\n",
       "             ('max_target_length', 2048),\n",
       "             ('max_eval_target_length', 512),\n",
       "             ('max_predict_length', 50),\n",
       "             ('sampling_temperature', 2.0),\n",
       "             ('sampling_top_k', 20),\n",
       "             ('eos_id', 2),\n",
       "             ('prompt', 'Can you explain neural network inference?'),\n",
       "             ('enable_profiler', False),\n",
       "             ('enable_checkpointing', True),\n",
       "             ('async_checkpointing', True),\n",
       "             ('enable_dropout', True),\n",
       "             ('enable_data_shuffling', True),\n",
       "             ('data_shuffle_seed', 0),\n",
       "             ('init_weights_seed', 0),\n",
       "             ('gradient_clipping_threshold', 1.0),\n",
       "             ('adam_b1', 0.9),\n",
       "             ('adam_b2', 0.95),\n",
       "             ('adam_eps', 1e-08),\n",
       "             ('adam_eps_root', 0.0),\n",
       "             ('adam_weight_decay', 0.1),\n",
       "             ('collect_stack_trace', False),\n",
       "             ('stack_trace_to_cloud', False),\n",
       "             ('stack_trace_interval_seconds', 600),\n",
       "             ('use_iota_embed', False),\n",
       "             ('compiled_trainstep_file', ''),\n",
       "             ('compile_topology', ''),\n",
       "             ('compile_topology_num_slices', -1),\n",
       "             ('tensorboard_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/2b/20231121/prompt-2b-v1/tensorboard/'),\n",
       "             ('checkpoint_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/2b/20231121/prompt-2b-v1/checkpoints/'),\n",
       "             ('metrics_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/2b/20231121/prompt-2b-v1/metrics/'),\n",
       "             ('emb_dim', 2560),\n",
       "             ('num_heads', 8),\n",
       "             ('mlp_dim', 8192),\n",
       "             ('num_decoder_layers', 32),\n",
       "             ('global_batch_size_to_load', 48),\n",
       "             ('global_batch_size_to_train_on', 48)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bucket = \"gs://patflick-maxtext-lingvo\"\n",
    "base_output_directory= my_bucket + \"/maxtext/2b/20231121\"\n",
    "\n",
    "checkpoint_path = \"gs://test-example-123/maxtest-lg/2b-c4/v4-32/maxtext_cnndm/checkpoints\"\n",
    "\n",
    "extra_config = {\n",
    "    # Run config\n",
    "    \"run_name\": \"prompt-2b-v1\",\n",
    "    \"base_output_directory\": base_output_directory,\n",
    "\n",
    "    # Load checkpoint\n",
    "    \"load_from_other_directory\": checkpoint_path,\n",
    "\n",
    "    # Parallelism\n",
    "    \"dcn_tensor_parallelism\": 4,\n",
    "    \"ici_tensor_parallelism\": 4,\n",
    "\n",
    "    # c4 dataset downloaded via `download_dataset.sh` from gs://allennlp-tensorflow-datasets/c4/en/3.0.1/\n",
    "    \"dataset_path\": my_bucket + \"/\",\n",
    "    \"assets_path\": \"/home/patflick/dev/git/maxtext/assets\", # TODO: move assets to GCS\n",
    "\n",
    "    # Config for `decode_loop`\n",
    "    \"prompt\": \"Can you explain neural network inference?\",\n",
    "    \"steps\": 10,\n",
    "    \"max_predict_length\": 50,\n",
    "\n",
    "    \"sampling_temperature\": 2.0\n",
    "}\n",
    "\n",
    "import pyconfig\n",
    "pyconfig.initialize([\"\", \"configs/base-2b.yml\"] + [f\"{k}={v}\" for k,v in extra_config.items()])\n",
    "config = pyconfig.config\n",
    "pyconfig._config.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] (num_devices: 4)\n",
      "Decided on mesh: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)\n",
      "   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]]]\n",
      "Model path: /home/patflick/dev/git/maxtext/assets/tokenizer\n"
     ]
    }
   ],
   "source": [
    "# MANUALLY load the training data and tokenizer\n",
    "import max_utils  \n",
    "import os\n",
    "from jax.sharding import Mesh\n",
    "from input_pipeline import create_data_iterator_with_tokenizer\n",
    "\n",
    "# Mesh definition\n",
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = Mesh(devices_array, config.mesh_axes)\n",
    "\n",
    "os.environ[\"TFDS_DATA_DIR\"] = pyconfig.config.dataset_path + \"/\"\n",
    "train_ds, sp_tokenizer = create_data_iterator_with_tokenizer(config, mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Tokenizer with vocab size of 32768\n",
      "Tokenized \"Hello world!\": [3869  190   46    2]\n",
      "Detokenized: b'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "# Try out tokenization / detokenization\n",
    "print(f\"Loaded Tokenizer with vocab size of {sp_tokenizer.vocab_size()}\")\n",
    "\n",
    "str = \"Hello world!\"\n",
    "tokenized = sp_tokenizer.tokenize(str)\n",
    "print(f\"Tokenized \\\"{str}\\\": {tokenized}\")\n",
    "\n",
    "detokenized = sp_tokenizer.detokenize(tokenized)\n",
    "print(f\"Detokenized: {detokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating checkpoint manager...\n",
      "Checkpoint manager created!\n",
      "Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] (num_devices: 4)\n",
      "Decided on mesh: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)\n",
      "   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]]]\n",
      "Model path: /home/patflick/dev/git/maxtext/assets/tokenizer\n",
      "restoring state from gs://test-example-123/maxtest-lg/2b-c4/v4-32/maxtext_cnndm/checkpoints latest step 16000\n"
     ]
    }
   ],
   "source": [
    "import decode\n",
    "import os\n",
    "import jax\n",
    "\n",
    "os.environ[\"TFDS_DATA_DIR\"] = pyconfig.config.dataset_path + \"/\"\n",
    "decode.decode_loop(config) \n",
    "jax.distributed.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import jax\n",
    "jax.distributed.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
