{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e892f7-e326-4e62-baf9-25a63fc38c9b",
   "metadata": {},
   "source": [
    "# MaxText Lingvo fine-tuning\n",
    "\n",
    "Loads a pre-existing checkpoint of the converged LG dummy model (single\n",
    "attention layer), and then runs some additional training steps (fine-tuning).\n",
    "\n",
    "### TODO:\n",
    "need to create/import all of:\n",
    "- [x] model: load from checkpoint\n",
    "- [x] tx: optimizer\n",
    "- [x] config\n",
    "- [x] init_rng\n",
    "- [x] mesh\n",
    "- [x] checkpoint_manager\n",
    "- [ ] basic training function\n",
    "- [ ] short version of this NB that uses just config and calls `train.train_loop`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd639915-a6b8-4c14-a3e6-27ef0dd5935b",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a434f0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('run_name', 'finetuning-dummy_v1'),\n",
       "             ('load_parameters_path', ''),\n",
       "             ('load_from_other_directory',\n",
       "              'gs://mazumdera-test-bucket/maxtext/lg/11032023/1/1xv3-8/checkpoints/'),\n",
       "             ('load_from_other_directory_step', 85),\n",
       "             ('reuse_example_batch', 0),\n",
       "             ('metrics_file', ''),\n",
       "             ('gcs_metrics', False),\n",
       "             ('dtype', dtype(bfloat16)),\n",
       "             ('int8_training', False),\n",
       "             ('global_parameter_scale', 1),\n",
       "             ('base_emb_dim', 512),\n",
       "             ('base_num_heads', 4),\n",
       "             ('base_mlp_dim', 2048),\n",
       "             ('base_num_decoder_layers', 1),\n",
       "             ('head_dim', 96),\n",
       "             ('mlp_activations', ['gelu']),\n",
       "             ('dropout_rate', 0),\n",
       "             ('logits_via_embedding', True),\n",
       "             ('remat_policy', 'full'),\n",
       "             ('scan_layers', True),\n",
       "             ('param_scan_axis', 1),\n",
       "             ('enable_flash_attention', False),\n",
       "             ('record_internal_nn_metrics', 0),\n",
       "             ('base_output_directory',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/dummy/20231120'),\n",
       "             ('mesh_axes', ['data', 'fsdp', 'tensor']),\n",
       "             ('logical_axis_rules',\n",
       "              (('activation_batch', ('data', 'fsdp')),\n",
       "               ('activation_length', ('data', 'fsdp')),\n",
       "               ('activation_embed', 'tensor'),\n",
       "               ('activation_mlp', 'tensor'),\n",
       "               ('activation_heads', 'tensor'),\n",
       "               ('activation_kv', 'tensor'),\n",
       "               ('activation_vocab', 'tensor'),\n",
       "               ('mlp', 'tensor'),\n",
       "               ('vocab', 'tensor'),\n",
       "               ('embed', 'fsdp'),\n",
       "               ('heads', 'tensor'))),\n",
       "             ('data_sharding', (('data', 'fsdp', 'tensor'),)),\n",
       "             ('dcn_data_parallelism', -1),\n",
       "             ('dcn_fsdp_parallelism', 1),\n",
       "             ('dcn_tensor_parallelism', 1),\n",
       "             ('ici_data_parallelism', 1),\n",
       "             ('ici_fsdp_parallelism', -1),\n",
       "             ('ici_tensor_parallelism', 4),\n",
       "             ('dataset_path', ''),\n",
       "             ('vocab_size', 50272),\n",
       "             ('assets_path', 'assets'),\n",
       "             ('vocab_relative_path', 'tokenizer'),\n",
       "             ('dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_split', 'validation'),\n",
       "             ('per_device_batch_size', 0.5),\n",
       "             ('eval_per_device_batch_size', 0),\n",
       "             ('max_corpus_chars', 10000000),\n",
       "             ('dataset_type', 'lg'),\n",
       "             ('file_pattern_for_train_data',\n",
       "              'gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords'),\n",
       "             ('file_pattern_for_eval_data',\n",
       "              'gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords'),\n",
       "             ('steps', 1000),\n",
       "             ('log_period', 100),\n",
       "             ('save_period', 100),\n",
       "             ('learning_rate', 3e-05),\n",
       "             ('cosine_learning_rate_final_fraction', 0.1),\n",
       "             ('warmup_steps_fraction', 0.1),\n",
       "             ('learning_rate_schedule_steps', 1000),\n",
       "             ('max_target_length', 2048),\n",
       "             ('max_eval_target_length', 512),\n",
       "             ('max_predict_length', 64),\n",
       "             ('sampling_temperature', 0.6),\n",
       "             ('sampling_top_k', 20),\n",
       "             ('eos_id', 2),\n",
       "             ('prompt', 'I love to '),\n",
       "             ('enable_profiler', False),\n",
       "             ('enable_checkpointing', True),\n",
       "             ('async_checkpointing', True),\n",
       "             ('enable_dropout', True),\n",
       "             ('enable_data_shuffling', True),\n",
       "             ('data_shuffle_seed', 0),\n",
       "             ('init_weights_seed', 0),\n",
       "             ('gradient_clipping_threshold', 1.0),\n",
       "             ('adam_b1', 0.9),\n",
       "             ('adam_b2', 0.95),\n",
       "             ('adam_eps', 1e-08),\n",
       "             ('adam_eps_root', 0.0),\n",
       "             ('adam_weight_decay', 0.1),\n",
       "             ('collect_stack_trace', False),\n",
       "             ('stack_trace_to_cloud', False),\n",
       "             ('stack_trace_interval_seconds', 600),\n",
       "             ('use_iota_embed', False),\n",
       "             ('tensorboard_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/dummy/20231120/finetuning-dummy_v1/tensorboard/'),\n",
       "             ('checkpoint_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/dummy/20231120/finetuning-dummy_v1/checkpoints/'),\n",
       "             ('metrics_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/dummy/20231120/finetuning-dummy_v1/metrics/'),\n",
       "             ('emb_dim', 512),\n",
       "             ('num_heads', 4),\n",
       "             ('mlp_dim', 2048),\n",
       "             ('num_decoder_layers', 1),\n",
       "             ('global_batch_size_to_load', 4),\n",
       "             ('global_batch_size_to_train_on', 2)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bucket = \"gs://patflick-maxtext-lingvo\"\n",
    "base_output_directory= my_bucket + \"/maxtext/dummy/20231120\"\n",
    "\n",
    "# Path to converted checkpoints\n",
    "checkpoint_path = \"gs://mazumdera-test-bucket/maxtext/lg/11032023/1/1xv3-8/checkpoints/\"\n",
    "\n",
    "# Adapt config to our model\n",
    "extra_config = {\n",
    "    # Run config\n",
    "    \"run_name\": \"finetuning-dummy_v1\",\n",
    "    \"base_output_directory\": base_output_directory,\n",
    "    \"save_period\": 100,\n",
    "    \"steps\": 1000,\n",
    "\n",
    "    # Load checkpoint\n",
    "    \"load_from_other_directory\": checkpoint_path,\n",
    "    \"load_from_other_directory_step\": 85,\n",
    "\n",
    "    # Model config (this has to match the loaded checkpoint, otherwise ERROR!)\n",
    "    \"base_num_decoder_layers\": 1,\n",
    "    \"base_num_heads\": 4,\n",
    "    \"head_dim\": 96,\n",
    "    \"vocab_size\": 50272,\n",
    "    \"per_device_batch_size\": 0.5,\n",
    "    \"base_mlp_dim\": 2048,\n",
    "    \"base_emb_dim\": 512,\n",
    "\n",
    "    # Dataset loader to use\n",
    "    \"dataset_type\": \"lg\",\n",
    "    \"file_pattern_for_train_data\": \"gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords\",\n",
    "    \"file_pattern_for_eval_data\": \"gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords\",\n",
    "\n",
    "    # Parallelism and KV config\n",
    "    \"dcn_tensor_parallelism\": 1,\n",
    "    \"ici_tensor_parallelism\": 4,\n",
    "    \"enable_flash_attention\": False,\n",
    "}\n",
    "\n",
    "import pyconfig\n",
    "pyconfig.initialize([\"\", \"configs/base.yml\"] + [f\"{k}={v}\" for k,v in extra_config.items()])\n",
    "config = pyconfig.config\n",
    "pyconfig._config.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ddd68f-8f01-4062-a0aa-67ecbdadcd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('run_name', '1xv4-8'),\n",
       "             ('load_parameters_path', ''),\n",
       "             ('load_from_other_directory',\n",
       "              'gs://mazumdera-test-bucket/maxtext/lg/11032023/1/1xv3-8/checkpoints/'),\n",
       "             ('load_from_other_directory_step', 50),\n",
       "             ('reuse_example_batch', 0),\n",
       "             ('metrics_file', ''),\n",
       "             ('gcs_metrics', False),\n",
       "             ('dtype', dtype(bfloat16)),\n",
       "             ('int8_training', False),\n",
       "             ('global_parameter_scale', 1),\n",
       "             ('base_emb_dim', 512),\n",
       "             ('base_num_heads', 4),\n",
       "             ('base_mlp_dim', 2048),\n",
       "             ('base_num_decoder_layers', 1),\n",
       "             ('head_dim', 96),\n",
       "             ('mlp_activations', ['gelu']),\n",
       "             ('dropout_rate', 0),\n",
       "             ('logits_via_embedding', True),\n",
       "             ('remat_policy', 'full'),\n",
       "             ('scan_layers', True),\n",
       "             ('param_scan_axis', 1),\n",
       "             ('enable_flash_attention', False),\n",
       "             ('record_internal_nn_metrics', 0),\n",
       "             ('base_output_directory',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/lingvo/20231108/1'),\n",
       "             ('mesh_axes', ['data', 'fsdp', 'tensor']),\n",
       "             ('logical_axis_rules',\n",
       "              (('activation_batch', ('data', 'fsdp')),\n",
       "               ('activation_length', ('data', 'fsdp')),\n",
       "               ('activation_embed', 'tensor'),\n",
       "               ('activation_mlp', 'tensor'),\n",
       "               ('activation_heads', 'tensor'),\n",
       "               ('activation_kv', 'tensor'),\n",
       "               ('activation_vocab', 'tensor'),\n",
       "               ('mlp', 'tensor'),\n",
       "               ('vocab', 'tensor'),\n",
       "               ('embed', 'fsdp'),\n",
       "               ('heads', 'tensor'))),\n",
       "             ('data_sharding', (('data', 'fsdp', 'tensor'),)),\n",
       "             ('dcn_data_parallelism', 1),\n",
       "             ('dcn_fsdp_parallelism', 1),\n",
       "             ('dcn_tensor_parallelism', 1),\n",
       "             ('ici_data_parallelism', 2),\n",
       "             ('ici_fsdp_parallelism', 1),\n",
       "             ('ici_tensor_parallelism', 2),\n",
       "             ('dataset_path', ''),\n",
       "             ('vocab_size', 50272),\n",
       "             ('assets_path', 'assets'),\n",
       "             ('vocab_relative_path', 'tokenizer'),\n",
       "             ('dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_split', 'validation'),\n",
       "             ('per_device_batch_size', 0.5),\n",
       "             ('eval_per_device_batch_size', 0),\n",
       "             ('max_corpus_chars', 10000000),\n",
       "             ('dataset_type', 'lg'),\n",
       "             ('file_pattern_for_train_data',\n",
       "              'gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords'),\n",
       "             ('file_pattern_for_eval_data',\n",
       "              'gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords'),\n",
       "             ('steps', 20),\n",
       "             ('log_period', 100),\n",
       "             ('save_period', 5),\n",
       "             ('learning_rate', 3e-05),\n",
       "             ('cosine_learning_rate_final_fraction', 0.1),\n",
       "             ('warmup_steps_fraction', 0.1),\n",
       "             ('learning_rate_schedule_steps', 20),\n",
       "             ('max_target_length', 2048),\n",
       "             ('max_eval_target_length', 512),\n",
       "             ('max_predict_length', 512),\n",
       "             ('sampling_temperature', 0.6),\n",
       "             ('sampling_top_k', 20),\n",
       "             ('eos_id', 2),\n",
       "             ('prompt', 'I love to '),\n",
       "             ('enable_profiler', True),\n",
       "             ('enable_checkpointing', True),\n",
       "             ('async_checkpointing', True),\n",
       "             ('enable_dropout', True),\n",
       "             ('enable_data_shuffling', True),\n",
       "             ('data_shuffle_seed', 0),\n",
       "             ('init_weights_seed', 0),\n",
       "             ('gradient_clipping_threshold', 1.0),\n",
       "             ('adam_b1', 0.9),\n",
       "             ('adam_b2', 0.95),\n",
       "             ('adam_eps', 1e-08),\n",
       "             ('adam_eps_root', 0.0),\n",
       "             ('adam_weight_decay', 0.1),\n",
       "             ('collect_stack_trace', False),\n",
       "             ('stack_trace_to_cloud', False),\n",
       "             ('stack_trace_interval_seconds', 600),\n",
       "             ('use_iota_embed', False),\n",
       "             ('tensorboard_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/lingvo/20231108/1/1xv4-8/tensorboard/'),\n",
       "             ('checkpoint_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/lingvo/20231108/1/1xv4-8/checkpoints/'),\n",
       "             ('metrics_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/lingvo/20231108/1/1xv4-8/metrics/'),\n",
       "             ('emb_dim', 512),\n",
       "             ('num_heads', 4),\n",
       "             ('mlp_dim', 2048),\n",
       "             ('num_decoder_layers', 1),\n",
       "             ('global_batch_size_to_load', 4),\n",
       "             ('global_batch_size_to_train_on', 2)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#config \n",
    "my_bucket = \"gs://patflick-maxtext-lingvo\"\n",
    "base_output_directory=my_bucket + \"/maxtext/lingvo/20231108/1\"\n",
    "\n",
    "# Aisha's checkpoints\n",
    "#load_checkpoint_dir=\"gs://mazumdera-test-bucket/maxtext/lg/10142023/1/1xv3-8/checkpoints/\"\n",
    "load_checkpoint_dir=\"gs://mazumdera-test-bucket/maxtext/lg/11032023/1/1xv3-8/checkpoints/\"\n",
    "#base_output_directory=\"base_output_directory=gs://mazumdera-test-bucket/maxtext/lg/11032023/1\"\n",
    "\n",
    "# Train/Eval data\n",
    "file_pattern_for_train_data=\"file_pattern_for_train_data=gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords\"\n",
    "file_pattern_for_eval_data=\"file_pattern_for_eval_data=gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords\"\n",
    "\n",
    "base_num_decoder_layers=\"base_num_decoder_layers=1\"\n",
    "base_num_heads = \"base_num_heads=4\"\n",
    "head_nums = \"head_dim=96\"\n",
    "dataset_type = \"dataset_type=lg\"\n",
    "\n",
    "commandline_args = [\"dummy\", \n",
    "                    \"configs/base.yml\",\n",
    "                    \"run_name=1xv4-8\",\n",
    "                    \"dcn_data_parallelism=1\",\n",
    "                    \"save_period=5\",\n",
    "                    # TODO: configure parallelism!\n",
    "                    \"ici_data_parallelism=2\",\n",
    "                    \"ici_tensor_parallelism=2\",\n",
    "                    \"ici_fsdp_parallelism=1\",\n",
    "                    \"steps=20\",\n",
    "                    \"enable_profiler=true\",\n",
    "                    \"remat_policy=full\",\n",
    "                    \"base_emb_dim=512\", \n",
    "                    base_num_heads,\n",
    "                    head_nums,\n",
    "                    \"vocab_size=50272\",\n",
    "                    base_num_decoder_layers,\n",
    "                    \"per_device_batch_size=0.5\",\n",
    "                    \"enable_profiler=true\",\n",
    "                    \"base_mlp_dim=2048\", \n",
    "                    # File dependencies\n",
    "                    file_pattern_for_train_data, \n",
    "                    file_pattern_for_eval_data,\n",
    "                    \"base_output_directory=\" + base_output_directory,\n",
    "                    \"load_from_other_directory=\" + load_checkpoint_dir,\n",
    "                    \"load_from_other_directory_step=50\",\n",
    "                    dataset_type,\n",
    "                    \"max_predict_length=512\",\n",
    "                    #\"jax_default_prng_impl=unsafe_rgb\"   # required/overwritten by train.train_step. if not set here, will cause failures later\n",
    "                   ]\n",
    "\n",
    "import pyconfig\n",
    "pyconfig.initialize(commandline_args)\n",
    "config = pyconfig.config\n",
    "pyconfig._config.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d80d0-ca2f-4300-805e-ea2f7d2f50ca",
   "metadata": {},
   "source": [
    "## Loading pre-existing checkpoint\n",
    "\n",
    "Uses the config to create the optimizer, model, and mesh. Then loads the checkpoint into the model/optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3a3355-0983-4638-be83-77dae97130a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating checkpoint manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 23:44:26.016643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint manager created!\n"
     ]
    }
   ],
   "source": [
    "import checkpointing\n",
    "checkpoint_manager = checkpointing.create_orbax_checkpoint_manager(\n",
    "      checkpoint_dir = config.checkpoint_dir,\n",
    "      enable_checkpointing = True,   # need to be true to allow loading other checkpints\n",
    "      use_async = config.async_checkpointing,\n",
    "      save_interval_steps = config.save_period\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabc3c29-c24f-4d73-b1bd-deafac3b9b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f663ef66-3c24-4327-91ec-745a6576bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] (num_devices: 4)\n",
      "Decided on mesh: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)\n",
      "   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mesh(device_ids=array([[[0, 2, 1, 3]]]), axis_names=('data', 'fsdp', 'tensor'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup device mesh\n",
    "import max_utils\n",
    "from jax.sharding import Mesh\n",
    "\n",
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = Mesh(devices_array, config.mesh_axes)\n",
    "mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff51a358-d6ab-4745-8b49-5a1b63cd5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "# Initial PRNG Keys\n",
    "jax.config.update('jax_default_prng_impl', 'unsafe_rbg')   # need to set here. train.py later sets this and then causes shape mismatch\n",
    "init_rng, nextrng = random.split(random.PRNGKey(config.init_weights_seed), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d047d60-1926-4148-aecd-cb1a9be2651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "from layers import Transformer\n",
    "model = Transformer(config, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1feb2085-fdc2-4aa9-a4e1-9c2f18ab36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "# create optimizer\n",
    "#TODO(from original notebook): also compare with optax.adafactor\n",
    "tx = optax.adamw(\n",
    "       max_utils.create_learning_rate_schedule(config),\n",
    "       b1=config.adam_b1,\n",
    "       b2=config.adam_b2,\n",
    "       eps=config.adam_eps,\n",
    "       eps_root=config.adam_eps_root,\n",
    "       weight_decay=config.adam_weight_decay,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0fe610-4ed1-461e-a020-9ffe7ff9eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring state from gs://mazumdera-test-bucket/maxtext/lg/11032023/1/1xv3-8/checkpoints/ step 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1700696741.957102 1183063 gcs_resource.cc:99] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1700696741.958659 1184610 google_auth_provider.cc:179] Running on GCE, using service account 903354779218-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint and initialize model\n",
    "state_read_from_ckpt, state_mesh_annotations_read_from_ckpt = max_utils.setup_initial_state(model, tx, config, init_rng, mesh, checkpoint_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70149528-8ed6-426a-9455-c35b846abd3c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Print out the loaded transformer model architecture. This currently is a simple fake model with a single self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7434bae2-58d2-431b-b0b9-bfed48eb0e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder:\n",
      "  decoder:\n",
      "    mlp:\n",
      "      wi:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1, 2048)\n",
      "      wo:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(2048, 1, 512)\n",
      "    pre_mlp_layer_norm:\n",
      "      scale: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1)\n",
      "    pre_self_attention_layer_norm:\n",
      "      scale: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1)\n",
      "    relpos_bias:\n",
      "      rel_embedding: <class 'jaxlib.xla_extension.ArrayImpl'>(4, 1, 32)\n",
      "    self_attention:\n",
      "      key:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1, 4, 96)\n",
      "      out:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(4, 1, 96, 512)\n",
      "      query:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1, 4, 96)\n",
      "      value:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1, 4, 96)\n",
      "  decoder_norm:\n",
      "    scale: <class 'jaxlib.xla_extension.ArrayImpl'>(512,)\n",
      "token_embedder:\n",
      "  embedding: <class 'jaxlib.xla_extension.ArrayImpl'>(50272, 512)\n"
     ]
    }
   ],
   "source": [
    "state = state_read_from_ckpt\n",
    "\n",
    "type(state.params)\n",
    "def print_params_shape(params):\n",
    "    def _print_params_dict_rec(sub_dict, ident=\"\"):\n",
    "        \n",
    "        for key, value in sub_dict.items():\n",
    "            if type(value) == dict:\n",
    "                print(f\"{ident}{value}:\")\n",
    "                _print_params_dict_rec(value, ident + \"  \")\n",
    "\n",
    "            line = key\n",
    "            if type(key) != str:\n",
    "                line = str(type(key))\n",
    "            if type(value) == dict:\n",
    "                print(ident + line + \":\")\n",
    "                _print_params_dict_rec(value, ident + \"  \")\n",
    "            elif \"shape\" in dir(value):\n",
    "                print(ident + line + \": \" + str(type(value)) + str(value.shape))\n",
    "            else:\n",
    "                print(ident + line + \": \" + str(type(value)))\n",
    "    _print_params_dict_rec(params)\n",
    "            \n",
    "print_params_shape(state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a20b9-3a67-4d92-af15-d1024e58d3d4",
   "metadata": {},
   "source": [
    "### Create sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61a22560-9328-4dc7-a19c-d69efbcae247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec as P\n",
    "\n",
    "# Compute sharding by combining checkpoint PartitionSpecs with config's mesh\n",
    "data_pspec = P(*config.data_sharding)\n",
    "state_mesh_shardings_read_from_ckpt = jax.tree_map(\n",
    "  lambda p: jax.sharding.NamedSharding(mesh, p), state_mesh_annotations_read_from_ckpt)\n",
    "data_sharding = jax.tree_map(\n",
    "  lambda p: jax.sharding.NamedSharding(mesh, p), data_pspec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7cc7f-c8c4-4027-b520-f776868b7b7a",
   "metadata": {},
   "source": [
    "## Continuing training on checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dce829",
   "metadata": {},
   "source": [
    "Loading input data processor/iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6f9ee03-7bec-4bd4-9534-8dbd4247d344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patflick/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read training data from path: gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords\n",
      "Training dataset has: 500000 entries\n",
      "Trying to read eval data from path: gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords\n",
      "Eval dataset has: 50000 entries\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "from input_pipeline import create_data_iterator_with_tokenizer\n",
    "\n",
    "data_iterator = create_data_iterator_with_tokenizer(config, mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607580b",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95826d5c-10f2-4a3a-86aa-76671df1d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.compilation_cache:Initialized persistent compilation cache at /home/patflick/jax_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 devices.\n",
      "train step 86 loss: 12.052555\n",
      "train step 87 loss: 11.998739\n",
      "train step 88 loss: 11.984926\n",
      "train step 89 loss: 11.960432\n",
      "train step 90 loss: 11.870367\n",
      "train step 91 loss: 11.914023\n",
      "train step 92 loss: 11.878504\n",
      "train step 93 loss: 11.812907\n",
      "train step 94 loss: 11.832667\n",
      "train step 95 loss: 11.78319\n",
      "train step 96 loss: 11.790079\n",
      "train step 97 loss: 11.736826\n",
      "train step 98 loss: 11.72094\n",
      "train step 99 loss: 11.696386\n",
      "train step 100 loss: 11.693748\n",
      "train step 101 loss: 11.651544\n",
      "train step 102 loss: 11.649\n",
      "train step 103 loss: 11.614431\n",
      "train step 104 loss: 11.627368\n",
      "train step 105 loss: 11.602772\n",
      "train step 106 loss: 11.591671\n",
      "train step 107 loss: 11.580237\n",
      "train step 108 loss: 11.540206\n",
      "train step 109 loss: 11.581579\n",
      "train step 110 loss: 11.553307\n",
      "train step 111 loss: 11.579695\n",
      "train step 112 loss: 11.568599\n",
      "train step 113 loss: 11.501836\n",
      "train step 114 loss: 11.530902\n",
      "train step 115 loss: 11.49118\n",
      "train step 116 loss: 11.50413\n",
      "train step 117 loss: 11.501064\n",
      "train step 118 loss: 11.482614\n",
      "train step 119 loss: 11.48354\n",
      "train step 120 loss: 11.433232\n",
      "train step 121 loss: 11.460157\n",
      "train step 122 loss: 11.473274\n",
      "train step 123 loss: 11.455875\n",
      "train step 124 loss: 11.450466\n",
      "train step 125 loss: 11.445099\n",
      "train step 126 loss: 11.445383\n",
      "train step 127 loss: 11.424355\n",
      "train step 128 loss: 11.4245205\n",
      "train step 129 loss: 11.433136\n",
      "train step 130 loss: 11.4249735\n",
      "train step 131 loss: 11.431505\n",
      "train step 132 loss: 11.431784\n",
      "train step 133 loss: 11.406978\n",
      "train step 134 loss: 11.399067\n",
      "train step 135 loss: 11.42319\n",
      "train step 136 loss: 11.400914\n",
      "train step 137 loss: 11.401341\n",
      "train step 138 loss: 11.42345\n",
      "train step 139 loss: 11.427195\n",
      "train step 140 loss: 11.359859\n",
      "train step 141 loss: 11.374327\n",
      "train step 142 loss: 11.365152\n",
      "train step 143 loss: 11.353081\n",
      "train step 144 loss: 11.398521\n",
      "train step 145 loss: 11.383778\n",
      "train step 146 loss: 11.402456\n",
      "train step 147 loss: 11.374388\n",
      "train step 148 loss: 11.362838\n",
      "train step 149 loss: 11.377861\n",
      "train step 150 loss: 11.360075\n",
      "train step 151 loss: 11.373919\n",
      "train step 152 loss: 11.3736515\n",
      "train step 153 loss: 11.385451\n",
      "train step 154 loss: 11.35885\n",
      "train step 155 loss: 11.386614\n",
      "train step 156 loss: 11.392112\n",
      "train step 157 loss: 11.384308\n",
      "train step 158 loss: 11.375584\n",
      "train step 159 loss: 11.39579\n",
      "train step 160 loss: 11.361889\n",
      "train step 161 loss: 11.364037\n",
      "train step 162 loss: 11.36627\n",
      "train step 163 loss: 11.373227\n",
      "train step 164 loss: 11.361155\n",
      "train step 165 loss: 11.35182\n",
      "train step 166 loss: 11.3662195\n",
      "train step 167 loss: 11.339636\n",
      "train step 168 loss: 11.350439\n",
      "train step 169 loss: 11.353645\n",
      "train step 170 loss: 11.345844\n",
      "train step 171 loss: 11.338588\n",
      "train step 172 loss: 11.3676605\n",
      "train step 173 loss: 11.339914\n",
      "train step 174 loss: 11.370921\n",
      "train step 175 loss: 11.349667\n",
      "train step 176 loss: 11.328678\n",
      "train step 177 loss: 11.3474455\n",
      "train step 178 loss: 11.339582\n",
      "train step 179 loss: 11.364728\n",
      "train step 180 loss: 11.3720875\n",
      "train step 181 loss: 11.355106\n",
      "train step 182 loss: 11.350874\n",
      "train step 183 loss: 11.329485\n",
      "train step 184 loss: 11.351733\n",
      "train step 185 loss: 11.330366\n",
      "train step 186 loss: 11.350734\n",
      "train step 187 loss: 11.377108\n",
      "train step 188 loss: 11.346012\n",
      "train step 189 loss: 11.339611\n",
      "train step 190 loss: 11.330294\n",
      "train step 191 loss: 11.348411\n",
      "train step 192 loss: 11.3120365\n",
      "train step 193 loss: 11.333926\n",
      "train step 194 loss: 11.379084\n",
      "train step 195 loss: 11.324387\n",
      "train step 196 loss: 11.325327\n",
      "train step 197 loss: 11.328886\n",
      "train step 198 loss: 11.361506\n",
      "train step 199 loss: 11.334907\n",
      "train step 200 loss: 11.347835\n",
      "train step 201 loss: 11.359201\n",
      "train step 202 loss: 11.366568\n",
      "train step 203 loss: 11.332157\n",
      "train step 204 loss: 11.354467\n",
      "train step 205 loss: 11.314863\n",
      "train step 206 loss: 11.336932\n",
      "train step 207 loss: 11.333901\n",
      "train step 208 loss: 11.331488\n",
      "train step 209 loss: 11.340778\n",
      "train step 210 loss: 11.336115\n",
      "train step 211 loss: 11.311142\n",
      "train step 212 loss: 11.319232\n",
      "train step 213 loss: 11.3384\n",
      "train step 214 loss: 11.320244\n",
      "train step 215 loss: 11.332561\n",
      "train step 216 loss: 11.324379\n",
      "train step 217 loss: 11.32826\n",
      "train step 218 loss: 11.351667\n",
      "train step 219 loss: 11.312408\n",
      "train step 220 loss: 11.327503\n",
      "train step 221 loss: 11.3213\n",
      "train step 222 loss: 11.334884\n",
      "train step 223 loss: 11.317062\n",
      "train step 224 loss: 11.315466\n",
      "train step 225 loss: 11.350153\n",
      "train step 226 loss: 11.34392\n",
      "train step 227 loss: 11.3402\n",
      "train step 228 loss: 11.370356\n",
      "train step 229 loss: 11.340758\n",
      "train step 230 loss: 11.33557\n",
      "train step 231 loss: 11.343187\n",
      "train step 232 loss: 11.279921\n",
      "train step 233 loss: 11.34182\n",
      "train step 234 loss: 11.332642\n",
      "train step 235 loss: 11.31481\n",
      "train step 236 loss: 11.310876\n",
      "train step 237 loss: 11.316868\n",
      "train step 238 loss: 11.28953\n",
      "train step 239 loss: 11.328328\n",
      "train step 240 loss: 11.299482\n",
      "train step 241 loss: 11.326321\n",
      "train step 242 loss: 11.347902\n",
      "train step 243 loss: 11.316304\n",
      "train step 244 loss: 11.326245\n",
      "train step 245 loss: 11.286505\n",
      "train step 246 loss: 11.314056\n",
      "train step 247 loss: 11.334845\n",
      "train step 248 loss: 11.321249\n",
      "train step 249 loss: 11.330204\n",
      "train step 250 loss: 11.323115\n",
      "train step 251 loss: 11.309742\n",
      "train step 252 loss: 11.295451\n",
      "train step 253 loss: 11.325802\n",
      "train step 254 loss: 11.307798\n",
      "train step 255 loss: 11.326777\n",
      "train step 256 loss: 11.323278\n",
      "train step 257 loss: 11.295883\n",
      "train step 258 loss: 11.311345\n",
      "train step 259 loss: 11.333082\n",
      "train step 260 loss: 11.276806\n",
      "train step 261 loss: 11.299017\n",
      "train step 262 loss: 11.311565\n",
      "train step 263 loss: 11.312565\n",
      "train step 264 loss: 11.306314\n",
      "train step 265 loss: 11.296865\n",
      "train step 266 loss: 11.318767\n",
      "train step 267 loss: 11.323755\n",
      "train step 268 loss: 11.322555\n",
      "train step 269 loss: 11.307185\n",
      "train step 270 loss: 11.336409\n",
      "train step 271 loss: 11.306682\n",
      "train step 272 loss: 11.292552\n",
      "train step 273 loss: 11.33275\n",
      "train step 274 loss: 11.344273\n",
      "train step 275 loss: 11.303631\n",
      "train step 276 loss: 11.331305\n",
      "train step 277 loss: 11.321188\n",
      "train step 278 loss: 11.335931\n",
      "train step 279 loss: 11.324116\n",
      "train step 280 loss: 11.285089\n",
      "train step 281 loss: 11.32996\n",
      "train step 282 loss: 11.298362\n",
      "train step 283 loss: 11.308381\n",
      "train step 284 loss: 11.342274\n",
      "train step 285 loss: 11.300454\n",
      "train step 286 loss: 11.306983\n",
      "train step 287 loss: 11.296383\n",
      "train step 288 loss: 11.287815\n",
      "train step 289 loss: 11.318703\n",
      "train step 290 loss: 11.296095\n",
      "train step 291 loss: 11.31423\n",
      "train step 292 loss: 11.322093\n",
      "train step 293 loss: 11.345311\n",
      "train step 294 loss: 11.280346\n",
      "train step 295 loss: 11.283026\n",
      "train step 296 loss: 11.312325\n",
      "train step 297 loss: 11.303617\n",
      "train step 298 loss: 11.294925\n",
      "train step 299 loss: 11.296035\n",
      "train step 300 loss: 11.315631\n",
      "train step 301 loss: 11.323097\n",
      "train step 302 loss: 11.281779\n",
      "train step 303 loss: 11.28667\n",
      "train step 304 loss: 11.303895\n",
      "train step 305 loss: 11.336465\n",
      "train step 306 loss: 11.332462\n",
      "train step 307 loss: 11.300404\n",
      "train step 308 loss: 11.281923\n",
      "train step 309 loss: 11.31391\n",
      "train step 310 loss: 11.319725\n",
      "train step 311 loss: 11.310013\n",
      "train step 312 loss: 11.298285\n",
      "train step 313 loss: 11.277765\n",
      "train step 314 loss: 11.312211\n",
      "train step 315 loss: 11.302127\n",
      "train step 316 loss: 11.317673\n",
      "train step 317 loss: 11.311071\n",
      "train step 318 loss: 11.290151\n",
      "train step 319 loss: 11.287726\n",
      "train step 320 loss: 11.30348\n",
      "train step 321 loss: 11.278646\n",
      "train step 322 loss: 11.29287\n",
      "train step 323 loss: 11.294515\n",
      "train step 324 loss: 11.317629\n",
      "train step 325 loss: 11.275984\n",
      "train step 326 loss: 11.262486\n",
      "train step 327 loss: 11.323715\n",
      "train step 328 loss: 11.308015\n",
      "train step 329 loss: 11.320267\n",
      "train step 330 loss: 11.303677\n",
      "train step 331 loss: 11.285826\n",
      "train step 332 loss: 11.309669\n",
      "train step 333 loss: 11.321619\n",
      "train step 334 loss: 11.286026\n",
      "train step 335 loss: 11.283756\n",
      "train step 336 loss: 11.277671\n",
      "train step 337 loss: 11.297644\n",
      "train step 338 loss: 11.2651005\n",
      "train step 339 loss: 11.314127\n",
      "train step 340 loss: 11.266695\n",
      "train step 341 loss: 11.276585\n",
      "train step 342 loss: 11.315914\n",
      "train step 343 loss: 11.302155\n",
      "train step 344 loss: 11.278106\n",
      "train step 345 loss: 11.322189\n",
      "train step 346 loss: 11.292173\n",
      "train step 347 loss: 11.297522\n",
      "train step 348 loss: 11.289953\n",
      "train step 349 loss: 11.281746\n",
      "train step 350 loss: 11.293855\n",
      "train step 351 loss: 11.303251\n",
      "train step 352 loss: 11.299496\n",
      "train step 353 loss: 11.286165\n",
      "train step 354 loss: 11.290054\n",
      "train step 355 loss: 11.283733\n",
      "train step 356 loss: 11.331888\n",
      "train step 357 loss: 11.286291\n",
      "train step 358 loss: 11.303171\n",
      "train step 359 loss: 11.304912\n",
      "train step 360 loss: 11.305523\n",
      "train step 361 loss: 11.290355\n",
      "train step 362 loss: 11.294728\n",
      "train step 363 loss: 11.2941265\n",
      "train step 364 loss: 11.299941\n",
      "train step 365 loss: 11.306804\n",
      "train step 366 loss: 11.282245\n",
      "train step 367 loss: 11.281761\n",
      "train step 368 loss: 11.302751\n",
      "train step 369 loss: 11.306204\n",
      "train step 370 loss: 11.276477\n",
      "train step 371 loss: 11.297173\n",
      "train step 372 loss: 11.307257\n",
      "train step 373 loss: 11.30727\n",
      "train step 374 loss: 11.285385\n",
      "train step 375 loss: 11.277658\n",
      "train step 376 loss: 11.31645\n",
      "train step 377 loss: 11.2862425\n",
      "train step 378 loss: 11.282179\n",
      "train step 379 loss: 11.252867\n",
      "train step 380 loss: 11.276775\n",
      "train step 381 loss: 11.308803\n",
      "train step 382 loss: 11.284263\n",
      "train step 383 loss: 11.288378\n",
      "train step 384 loss: 11.302204\n",
      "train step 385 loss: 11.283472\n",
      "train step 386 loss: 11.307214\n",
      "train step 387 loss: 11.26622\n",
      "train step 388 loss: 11.282072\n",
      "train step 389 loss: 11.304038\n",
      "train step 390 loss: 11.282866\n",
      "train step 391 loss: 11.2907\n",
      "train step 392 loss: 11.273115\n",
      "train step 393 loss: 11.292957\n",
      "train step 394 loss: 11.324234\n",
      "train step 395 loss: 11.2935505\n",
      "train step 396 loss: 11.293562\n",
      "train step 397 loss: 11.303383\n",
      "train step 398 loss: 11.296915\n",
      "train step 399 loss: 11.295383\n",
      "train step 400 loss: 11.292871\n",
      "train step 401 loss: 11.31432\n",
      "train step 402 loss: 11.288239\n",
      "train step 403 loss: 11.289175\n",
      "train step 404 loss: 11.266846\n",
      "train step 405 loss: 11.293433\n",
      "train step 406 loss: 11.28792\n",
      "train step 407 loss: 11.278751\n",
      "train step 408 loss: 11.282146\n",
      "train step 409 loss: 11.273198\n",
      "train step 410 loss: 11.283686\n",
      "train step 411 loss: 11.26816\n",
      "train step 412 loss: 11.274919\n",
      "train step 413 loss: 11.283821\n",
      "train step 414 loss: 11.270503\n",
      "train step 415 loss: 11.27751\n",
      "train step 416 loss: 11.284569\n",
      "train step 417 loss: 11.278228\n",
      "train step 418 loss: 11.288489\n",
      "train step 419 loss: 11.2631645\n",
      "train step 420 loss: 11.286834\n",
      "train step 421 loss: 11.253312\n",
      "train step 422 loss: 11.271989\n",
      "train step 423 loss: 11.26767\n",
      "train step 424 loss: 11.307112\n",
      "train step 425 loss: 11.297411\n",
      "train step 426 loss: 11.291906\n",
      "train step 427 loss: 11.281081\n",
      "train step 428 loss: 11.256249\n",
      "train step 429 loss: 11.275076\n",
      "train step 430 loss: 11.276547\n",
      "train step 431 loss: 11.272443\n",
      "train step 432 loss: 11.276852\n",
      "train step 433 loss: 11.254452\n",
      "train step 434 loss: 11.270861\n",
      "train step 435 loss: 11.277386\n",
      "train step 436 loss: 11.275386\n",
      "train step 437 loss: 11.2944765\n",
      "train step 438 loss: 11.269403\n",
      "train step 439 loss: 11.297245\n",
      "train step 440 loss: 11.2719\n",
      "train step 441 loss: 11.285433\n",
      "train step 442 loss: 11.270895\n",
      "train step 443 loss: 11.277025\n",
      "train step 444 loss: 11.3006735\n",
      "train step 445 loss: 11.284319\n",
      "train step 446 loss: 11.277893\n",
      "train step 447 loss: 11.263376\n",
      "train step 448 loss: 11.274323\n",
      "train step 449 loss: 11.255646\n",
      "train step 450 loss: 11.24653\n",
      "train step 451 loss: 11.262608\n",
      "train step 452 loss: 11.295899\n",
      "train step 453 loss: 11.279497\n",
      "train step 454 loss: 11.270371\n",
      "train step 455 loss: 11.254957\n",
      "train step 456 loss: 11.283857\n",
      "train step 457 loss: 11.277843\n",
      "train step 458 loss: 11.269276\n",
      "train step 459 loss: 11.266943\n",
      "train step 460 loss: 11.269548\n",
      "train step 461 loss: 11.263235\n",
      "train step 462 loss: 11.261366\n",
      "train step 463 loss: 11.255604\n",
      "train step 464 loss: 11.297596\n",
      "train step 465 loss: 11.278038\n",
      "train step 466 loss: 11.283129\n",
      "train step 467 loss: 11.252806\n",
      "train step 468 loss: 11.287812\n",
      "train step 469 loss: 11.270397\n",
      "train step 470 loss: 11.279564\n",
      "train step 471 loss: 11.251989\n",
      "train step 472 loss: 11.280136\n",
      "train step 473 loss: 11.264924\n",
      "train step 474 loss: 11.27904\n",
      "train step 475 loss: 11.285438\n",
      "train step 476 loss: 11.286842\n",
      "train step 477 loss: 11.27563\n",
      "train step 478 loss: 11.276861\n",
      "train step 479 loss: 11.259199\n",
      "train step 480 loss: 11.302178\n",
      "train step 481 loss: 11.2745285\n",
      "train step 482 loss: 11.250585\n",
      "train step 483 loss: 11.3004\n",
      "train step 484 loss: 11.261198\n",
      "train step 485 loss: 11.310653\n",
      "train step 486 loss: 11.296133\n",
      "train step 487 loss: 11.272214\n",
      "train step 488 loss: 11.2821\n",
      "train step 489 loss: 11.2595625\n",
      "train step 490 loss: 11.270136\n",
      "train step 491 loss: 11.252775\n",
      "train step 492 loss: 11.267332\n",
      "train step 493 loss: 11.276518\n",
      "train step 494 loss: 11.266419\n",
      "train step 495 loss: 11.287732\n",
      "train step 496 loss: 11.251338\n",
      "train step 497 loss: 11.28567\n",
      "train step 498 loss: 11.270095\n",
      "train step 499 loss: 11.27049\n",
      "train step 500 loss: 11.240255\n",
      "train step 501 loss: 11.2672615\n",
      "train step 502 loss: 11.283674\n",
      "train step 503 loss: 11.27553\n",
      "train step 504 loss: 11.24749\n",
      "train step 505 loss: 11.266363\n",
      "train step 506 loss: 11.27004\n",
      "train step 507 loss: 11.253445\n",
      "train step 508 loss: 11.283812\n",
      "train step 509 loss: 11.255613\n",
      "train step 510 loss: 11.254602\n",
      "train step 511 loss: 11.277633\n",
      "train step 512 loss: 11.260639\n",
      "train step 513 loss: 11.262688\n",
      "train step 514 loss: 11.278209\n",
      "train step 515 loss: 11.3134\n",
      "train step 516 loss: 11.27728\n",
      "train step 517 loss: 11.289164\n",
      "train step 518 loss: 11.278902\n",
      "train step 519 loss: 11.284651\n",
      "train step 520 loss: 11.282722\n",
      "train step 521 loss: 11.246456\n",
      "train step 522 loss: 11.280858\n",
      "train step 523 loss: 11.271097\n",
      "train step 524 loss: 11.26052\n",
      "train step 525 loss: 11.245495\n",
      "train step 526 loss: 11.2354555\n",
      "train step 527 loss: 11.261827\n",
      "train step 528 loss: 11.258688\n",
      "train step 529 loss: 11.262577\n",
      "train step 530 loss: 11.269043\n",
      "train step 531 loss: 11.263973\n",
      "train step 532 loss: 11.272388\n",
      "train step 533 loss: 11.284193\n",
      "train step 534 loss: 11.300608\n",
      "train step 535 loss: 11.280812\n",
      "train step 536 loss: 11.289345\n",
      "train step 537 loss: 11.257255\n",
      "train step 538 loss: 11.260292\n",
      "train step 539 loss: 11.2801895\n",
      "train step 540 loss: 11.253258\n",
      "train step 541 loss: 11.2627125\n",
      "train step 542 loss: 11.279618\n",
      "train step 543 loss: 11.292995\n",
      "train step 544 loss: 11.2878065\n",
      "train step 545 loss: 11.267555\n",
      "train step 546 loss: 11.277884\n",
      "train step 547 loss: 11.261864\n",
      "train step 548 loss: 11.267214\n",
      "train step 549 loss: 11.245234\n",
      "train step 550 loss: 11.297108\n",
      "train step 551 loss: 11.285872\n",
      "train step 552 loss: 11.263954\n",
      "train step 553 loss: 11.254832\n",
      "train step 554 loss: 11.242825\n",
      "train step 555 loss: 11.287115\n",
      "train step 556 loss: 11.279993\n",
      "train step 557 loss: 11.251139\n",
      "train step 558 loss: 11.2674265\n",
      "train step 559 loss: 11.281334\n",
      "train step 560 loss: 11.287029\n",
      "train step 561 loss: 11.282671\n",
      "train step 562 loss: 11.293621\n",
      "train step 563 loss: 11.24658\n",
      "train step 564 loss: 11.255875\n",
      "train step 565 loss: 11.253014\n",
      "train step 566 loss: 11.272369\n",
      "train step 567 loss: 11.259852\n",
      "train step 568 loss: 11.268999\n",
      "train step 569 loss: 11.283671\n",
      "train step 570 loss: 11.287335\n",
      "train step 571 loss: 11.264401\n",
      "train step 572 loss: 11.256477\n",
      "train step 573 loss: 11.274992\n",
      "train step 574 loss: 11.252302\n",
      "train step 575 loss: 11.268243\n",
      "train step 576 loss: 11.2628\n",
      "train step 577 loss: 11.228786\n",
      "train step 578 loss: 11.254493\n",
      "train step 579 loss: 11.241975\n",
      "train step 580 loss: 11.246595\n",
      "train step 581 loss: 11.255428\n",
      "train step 582 loss: 11.263996\n",
      "train step 583 loss: 11.246473\n",
      "train step 584 loss: 11.241758\n",
      "train step 585 loss: 11.267618\n",
      "train step 586 loss: 11.231506\n",
      "train step 587 loss: 11.250198\n",
      "train step 588 loss: 11.251593\n",
      "train step 589 loss: 11.257062\n",
      "train step 590 loss: 11.237991\n",
      "train step 591 loss: 11.248463\n",
      "train step 592 loss: 11.267541\n",
      "train step 593 loss: 11.249998\n",
      "train step 594 loss: 11.267258\n",
      "train step 595 loss: 11.263653\n",
      "train step 596 loss: 11.267518\n",
      "train step 597 loss: 11.251882\n",
      "train step 598 loss: 11.27084\n",
      "train step 599 loss: 11.257623\n",
      "train step 600 loss: 11.274261\n",
      "train step 601 loss: 11.2604265\n",
      "train step 602 loss: 11.253424\n",
      "train step 603 loss: 11.265101\n",
      "train step 604 loss: 11.268471\n",
      "train step 605 loss: 11.230395\n",
      "train step 606 loss: 11.268314\n",
      "train step 607 loss: 11.233999\n",
      "train step 608 loss: 11.262432\n",
      "train step 609 loss: 11.216275\n",
      "train step 610 loss: 11.279465\n",
      "train step 611 loss: 11.271549\n",
      "train step 612 loss: 11.244787\n",
      "train step 613 loss: 11.248613\n",
      "train step 614 loss: 11.239567\n",
      "train step 615 loss: 11.265338\n",
      "train step 616 loss: 11.264618\n",
      "train step 617 loss: 11.2516\n",
      "train step 618 loss: 11.262947\n",
      "train step 619 loss: 11.241869\n",
      "train step 620 loss: 11.262755\n",
      "train step 621 loss: 11.269115\n",
      "train step 622 loss: 11.242467\n",
      "train step 623 loss: 11.262348\n",
      "train step 624 loss: 11.258295\n",
      "train step 625 loss: 11.245831\n",
      "train step 626 loss: 11.25459\n",
      "train step 627 loss: 11.236961\n",
      "train step 628 loss: 11.25613\n",
      "train step 629 loss: 11.24909\n",
      "train step 630 loss: 11.246261\n",
      "train step 631 loss: 11.289251\n",
      "train step 632 loss: 11.248149\n",
      "train step 633 loss: 11.2676735\n",
      "train step 634 loss: 11.283346\n",
      "train step 635 loss: 11.262606\n",
      "train step 636 loss: 11.2532835\n",
      "train step 637 loss: 11.23129\n",
      "train step 638 loss: 11.254487\n",
      "train step 639 loss: 11.245593\n",
      "train step 640 loss: 11.261248\n",
      "train step 641 loss: 11.243004\n",
      "train step 642 loss: 11.228003\n",
      "train step 643 loss: 11.247739\n",
      "train step 644 loss: 11.259388\n",
      "train step 645 loss: 11.256491\n",
      "train step 646 loss: 11.283426\n",
      "train step 647 loss: 11.257228\n",
      "train step 648 loss: 11.266279\n",
      "train step 649 loss: 11.251915\n",
      "train step 650 loss: 11.266956\n",
      "train step 651 loss: 11.28644\n",
      "train step 652 loss: 11.239403\n",
      "train step 653 loss: 11.249908\n",
      "train step 654 loss: 11.255329\n",
      "train step 655 loss: 11.252116\n",
      "train step 656 loss: 11.250836\n",
      "train step 657 loss: 11.264778\n",
      "train step 658 loss: 11.249382\n",
      "train step 659 loss: 11.240057\n",
      "train step 660 loss: 11.25906\n",
      "train step 661 loss: 11.243112\n",
      "train step 662 loss: 11.2749\n",
      "train step 663 loss: 11.262147\n",
      "train step 664 loss: 11.2622\n",
      "train step 665 loss: 11.232115\n",
      "train step 666 loss: 11.254381\n",
      "train step 667 loss: 11.246257\n",
      "train step 668 loss: 11.266543\n",
      "train step 669 loss: 11.240774\n",
      "train step 670 loss: 11.247124\n",
      "train step 671 loss: 11.260138\n",
      "train step 672 loss: 11.269533\n",
      "train step 673 loss: 11.250841\n",
      "train step 674 loss: 11.250591\n",
      "train step 675 loss: 11.253023\n",
      "train step 676 loss: 11.257747\n",
      "train step 677 loss: 11.267805\n",
      "train step 678 loss: 11.228849\n",
      "train step 679 loss: 11.241265\n",
      "train step 680 loss: 11.252381\n",
      "train step 681 loss: 11.275314\n",
      "train step 682 loss: 11.264263\n",
      "train step 683 loss: 11.248754\n",
      "train step 684 loss: 11.261127\n",
      "train step 685 loss: 11.269817\n",
      "train step 686 loss: 11.246482\n",
      "train step 687 loss: 11.241652\n",
      "train step 688 loss: 11.236341\n",
      "train step 689 loss: 11.23908\n",
      "train step 690 loss: 11.268296\n",
      "train step 691 loss: 11.257347\n",
      "train step 692 loss: 11.227148\n",
      "train step 693 loss: 11.275476\n",
      "train step 694 loss: 11.266327\n",
      "train step 695 loss: 11.26886\n",
      "train step 696 loss: 11.264711\n",
      "train step 697 loss: 11.255053\n",
      "train step 698 loss: 11.27059\n",
      "train step 699 loss: 11.245941\n",
      "train step 700 loss: 11.256893\n",
      "train step 701 loss: 11.2511425\n",
      "train step 702 loss: 11.250357\n",
      "train step 703 loss: 11.240709\n",
      "train step 704 loss: 11.2337\n",
      "train step 705 loss: 11.259342\n",
      "train step 706 loss: 11.263948\n",
      "train step 707 loss: 11.249937\n",
      "train step 708 loss: 11.275088\n",
      "train step 709 loss: 11.248138\n",
      "train step 710 loss: 11.25723\n",
      "train step 711 loss: 11.265487\n",
      "train step 712 loss: 11.254353\n",
      "train step 713 loss: 11.25177\n",
      "train step 714 loss: 11.240915\n",
      "train step 715 loss: 11.247814\n",
      "train step 716 loss: 11.243626\n",
      "train step 717 loss: 11.236943\n",
      "train step 718 loss: 11.261961\n",
      "train step 719 loss: 11.264218\n",
      "train step 720 loss: 11.234567\n",
      "train step 721 loss: 11.270401\n",
      "train step 722 loss: 11.2633\n",
      "train step 723 loss: 11.259465\n",
      "train step 724 loss: 11.24665\n",
      "train step 725 loss: 11.246162\n",
      "train step 726 loss: 11.2456665\n",
      "train step 727 loss: 11.25567\n",
      "train step 728 loss: 11.250341\n",
      "train step 729 loss: 11.23824\n",
      "train step 730 loss: 11.263386\n",
      "train step 731 loss: 11.254356\n",
      "train step 732 loss: 11.258678\n",
      "train step 733 loss: 11.244162\n",
      "train step 734 loss: 11.257936\n",
      "train step 735 loss: 11.23973\n",
      "train step 736 loss: 11.253437\n",
      "train step 737 loss: 11.224485\n",
      "train step 738 loss: 11.268072\n",
      "train step 739 loss: 11.256334\n",
      "train step 740 loss: 11.246201\n",
      "train step 741 loss: 11.264886\n",
      "train step 742 loss: 11.249312\n",
      "train step 743 loss: 11.247643\n",
      "train step 744 loss: 11.238108\n",
      "train step 745 loss: 11.23213\n",
      "train step 746 loss: 11.23612\n",
      "train step 747 loss: 11.247385\n",
      "train step 748 loss: 11.238967\n",
      "train step 749 loss: 11.247711\n",
      "train step 750 loss: 11.252633\n",
      "train step 751 loss: 11.225571\n",
      "train step 752 loss: 11.248661\n",
      "train step 753 loss: 11.252431\n",
      "train step 754 loss: 11.246209\n",
      "train step 755 loss: 11.255352\n",
      "train step 756 loss: 11.252375\n",
      "train step 757 loss: 11.252248\n",
      "train step 758 loss: 11.252314\n",
      "train step 759 loss: 11.229981\n",
      "train step 760 loss: 11.231714\n",
      "train step 761 loss: 11.237761\n",
      "train step 762 loss: 11.27169\n",
      "train step 763 loss: 11.269081\n",
      "train step 764 loss: 11.220323\n",
      "train step 765 loss: 11.251289\n",
      "train step 766 loss: 11.256622\n",
      "train step 767 loss: 11.250109\n",
      "train step 768 loss: 11.236767\n",
      "train step 769 loss: 11.2391205\n",
      "train step 770 loss: 11.234945\n",
      "train step 771 loss: 11.246681\n",
      "train step 772 loss: 11.22515\n",
      "train step 773 loss: 11.247955\n",
      "train step 774 loss: 11.26529\n",
      "train step 775 loss: 11.239959\n",
      "train step 776 loss: 11.240559\n",
      "train step 777 loss: 11.263008\n",
      "train step 778 loss: 11.263262\n",
      "train step 779 loss: 11.24569\n",
      "train step 780 loss: 11.271238\n",
      "train step 781 loss: 11.241178\n",
      "train step 782 loss: 11.271576\n",
      "train step 783 loss: 11.250398\n",
      "train step 784 loss: 11.231632\n",
      "train step 785 loss: 11.252567\n",
      "train step 786 loss: 11.235397\n",
      "train step 787 loss: 11.244356\n",
      "train step 788 loss: 11.2510395\n",
      "train step 789 loss: 11.242449\n",
      "train step 790 loss: 11.250622\n",
      "train step 791 loss: 11.243669\n",
      "train step 792 loss: 11.278445\n",
      "train step 793 loss: 11.260788\n",
      "train step 794 loss: 11.256091\n",
      "train step 795 loss: 11.265815\n",
      "train step 796 loss: 11.267145\n",
      "train step 797 loss: 11.231708\n",
      "train step 798 loss: 11.24016\n",
      "train step 799 loss: 11.242088\n",
      "train step 800 loss: 11.22042\n",
      "train step 801 loss: 11.230257\n",
      "train step 802 loss: 11.245236\n",
      "train step 803 loss: 11.257926\n",
      "train step 804 loss: 11.258331\n",
      "train step 805 loss: 11.269493\n",
      "train step 806 loss: 11.247608\n",
      "train step 807 loss: 11.263945\n",
      "train step 808 loss: 11.235168\n",
      "train step 809 loss: 11.246336\n",
      "train step 810 loss: 11.239733\n",
      "train step 811 loss: 11.225826\n",
      "train step 812 loss: 11.2414665\n",
      "train step 813 loss: 11.240889\n",
      "train step 814 loss: 11.232432\n",
      "train step 815 loss: 11.274496\n",
      "train step 816 loss: 11.24773\n",
      "train step 817 loss: 11.266575\n",
      "train step 818 loss: 11.254072\n",
      "train step 819 loss: 11.241783\n",
      "train step 820 loss: 11.237282\n",
      "train step 821 loss: 11.259193\n",
      "train step 822 loss: 11.225868\n",
      "train step 823 loss: 11.255299\n",
      "train step 824 loss: 11.252298\n",
      "train step 825 loss: 11.251346\n",
      "train step 826 loss: 11.253209\n",
      "train step 827 loss: 11.241266\n",
      "train step 828 loss: 11.250175\n",
      "train step 829 loss: 11.239758\n",
      "train step 830 loss: 11.246954\n",
      "train step 831 loss: 11.245056\n",
      "train step 832 loss: 11.23147\n",
      "train step 833 loss: 11.256236\n",
      "train step 834 loss: 11.244571\n",
      "train step 835 loss: 11.267537\n",
      "train step 836 loss: 11.235262\n",
      "train step 837 loss: 11.231543\n",
      "train step 838 loss: 11.237315\n",
      "train step 839 loss: 11.238348\n",
      "train step 840 loss: 11.255095\n",
      "train step 841 loss: 11.248575\n",
      "train step 842 loss: 11.257268\n",
      "train step 843 loss: 11.235998\n",
      "train step 844 loss: 11.257006\n",
      "train step 845 loss: 11.254686\n",
      "train step 846 loss: 11.256357\n",
      "train step 847 loss: 11.256504\n",
      "train step 848 loss: 11.248096\n",
      "train step 849 loss: 11.238857\n",
      "train step 850 loss: 11.239766\n",
      "train step 851 loss: 11.258261\n",
      "train step 852 loss: 11.251215\n",
      "train step 853 loss: 11.254243\n",
      "train step 854 loss: 11.243738\n",
      "train step 855 loss: 11.244347\n",
      "train step 856 loss: 11.230279\n",
      "train step 857 loss: 11.25352\n",
      "train step 858 loss: 11.263119\n",
      "train step 859 loss: 11.247732\n",
      "train step 860 loss: 11.253786\n",
      "train step 861 loss: 11.240971\n",
      "train step 862 loss: 11.269701\n",
      "train step 863 loss: 11.261502\n",
      "train step 864 loss: 11.253717\n",
      "train step 865 loss: 11.248298\n",
      "train step 866 loss: 11.262947\n",
      "train step 867 loss: 11.24103\n",
      "train step 868 loss: 11.245424\n",
      "train step 869 loss: 11.248501\n",
      "train step 870 loss: 11.245601\n",
      "train step 871 loss: 11.249792\n",
      "train step 872 loss: 11.244681\n",
      "train step 873 loss: 11.216405\n",
      "train step 874 loss: 11.252735\n",
      "train step 875 loss: 11.256178\n",
      "train step 876 loss: 11.261681\n",
      "train step 877 loss: 11.243577\n",
      "train step 878 loss: 11.261884\n",
      "train step 879 loss: 11.255693\n",
      "train step 880 loss: 11.236548\n",
      "train step 881 loss: 11.228738\n",
      "train step 882 loss: 11.257596\n",
      "train step 883 loss: 11.257745\n",
      "train step 884 loss: 11.225379\n",
      "train step 885 loss: 11.2375345\n",
      "train step 886 loss: 11.255039\n",
      "train step 887 loss: 11.229823\n",
      "train step 888 loss: 11.2526045\n",
      "train step 889 loss: 11.248065\n",
      "train step 890 loss: 11.244127\n",
      "train step 891 loss: 11.245001\n",
      "train step 892 loss: 11.261077\n",
      "train step 893 loss: 11.257003\n",
      "train step 894 loss: 11.244316\n",
      "train step 895 loss: 11.217096\n",
      "train step 896 loss: 11.238833\n",
      "train step 897 loss: 11.264563\n",
      "train step 898 loss: 11.266393\n",
      "train step 899 loss: 11.248703\n",
      "train step 900 loss: 11.252603\n",
      "train step 901 loss: 11.223528\n",
      "train step 902 loss: 11.224805\n",
      "train step 903 loss: 11.256308\n",
      "train step 904 loss: 11.257717\n",
      "train step 905 loss: 11.262507\n",
      "train step 906 loss: 11.249804\n",
      "train step 907 loss: 11.246271\n",
      "train step 908 loss: 11.242422\n",
      "train step 909 loss: 11.23741\n",
      "train step 910 loss: 11.2506485\n",
      "train step 911 loss: 11.2687435\n",
      "train step 912 loss: 11.223894\n",
      "train step 913 loss: 11.264702\n",
      "train step 914 loss: 11.26832\n",
      "train step 915 loss: 11.252844\n",
      "train step 916 loss: 11.258705\n",
      "train step 917 loss: 11.250104\n",
      "train step 918 loss: 11.232489\n",
      "train step 919 loss: 11.218723\n",
      "train step 920 loss: 11.239784\n",
      "train step 921 loss: 11.263138\n",
      "train step 922 loss: 11.262403\n",
      "train step 923 loss: 11.258564\n",
      "train step 924 loss: 11.245726\n",
      "train step 925 loss: 11.238259\n",
      "train step 926 loss: 11.2380295\n",
      "train step 927 loss: 11.254296\n",
      "train step 928 loss: 11.2477255\n",
      "train step 929 loss: 11.275408\n",
      "train step 930 loss: 11.243956\n",
      "train step 931 loss: 11.2512665\n",
      "train step 932 loss: 11.232063\n",
      "train step 933 loss: 11.254148\n",
      "train step 934 loss: 11.248947\n",
      "train step 935 loss: 11.258672\n",
      "train step 936 loss: 11.247462\n",
      "train step 937 loss: 11.237603\n",
      "train step 938 loss: 11.232567\n",
      "train step 939 loss: 11.269496\n",
      "train step 940 loss: 11.235767\n",
      "train step 941 loss: 11.224837\n",
      "train step 942 loss: 11.2738905\n",
      "train step 943 loss: 11.236906\n",
      "train step 944 loss: 11.242632\n",
      "train step 945 loss: 11.235712\n",
      "train step 946 loss: 11.255305\n",
      "train step 947 loss: 11.280041\n",
      "train step 948 loss: 11.234085\n",
      "train step 949 loss: 11.264393\n",
      "train step 950 loss: 11.246498\n",
      "train step 951 loss: 11.234808\n",
      "train step 952 loss: 11.243254\n",
      "train step 953 loss: 11.241623\n",
      "train step 954 loss: 11.237189\n",
      "train step 955 loss: 11.266957\n",
      "train step 956 loss: 11.2507925\n",
      "train step 957 loss: 11.245234\n",
      "train step 958 loss: 11.245633\n",
      "train step 959 loss: 11.238499\n",
      "train step 960 loss: 11.233449\n",
      "train step 961 loss: 11.248108\n",
      "train step 962 loss: 11.228098\n",
      "train step 963 loss: 11.246143\n",
      "train step 964 loss: 11.228357\n",
      "train step 965 loss: 11.249876\n",
      "train step 966 loss: 11.255806\n",
      "train step 967 loss: 11.229647\n",
      "train step 968 loss: 11.2550535\n",
      "train step 969 loss: 11.265346\n",
      "train step 970 loss: 11.234659\n",
      "train step 971 loss: 11.261588\n",
      "train step 972 loss: 11.225898\n",
      "train step 973 loss: 11.26717\n",
      "train step 974 loss: 11.22835\n",
      "train step 975 loss: 11.2671175\n",
      "train step 976 loss: 11.252193\n",
      "train step 977 loss: 11.236714\n",
      "train step 978 loss: 11.256168\n",
      "train step 979 loss: 11.243616\n",
      "train step 980 loss: 11.247575\n",
      "train step 981 loss: 11.231329\n",
      "train step 982 loss: 11.247665\n",
      "train step 983 loss: 11.242884\n",
      "train step 984 loss: 11.242872\n",
      "train step 985 loss: 11.239663\n",
      "train step 986 loss: 11.271181\n",
      "train step 987 loss: 11.230448\n",
      "train step 988 loss: 11.241131\n",
      "train step 989 loss: 11.224804\n",
      "train step 990 loss: 11.235254\n",
      "train step 991 loss: 11.212982\n",
      "train step 992 loss: 11.236958\n",
      "train step 993 loss: 11.259196\n",
      "train step 994 loss: 11.249573\n",
      "train step 995 loss: 11.233882\n",
      "train step 996 loss: 11.246216\n",
      "train step 997 loss: 11.256187\n",
      "train step 998 loss: 11.2458515\n",
      "train step 999 loss: 11.242104\n"
     ]
    }
   ],
   "source": [
    "import train\n",
    "import numpy as np\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "\n",
    "# compile train function\n",
    "p_train_step = jax.jit(train.train_step,\n",
    "                       in_shardings=(state_mesh_shardings_read_from_ckpt, data_sharding, None),\n",
    "                       out_shardings=(state_mesh_shardings_read_from_ckpt, None, None),\n",
    "                       static_argnums=(0,1,),\n",
    "                       donate_argnums=2)\n",
    "\n",
    "# run fine-tuning training\n",
    "batch = None\n",
    "for step in np.arange(train.get_first_step(state), 1000):\n",
    "    # load batch\n",
    "    batch = train.load_next_batch(data_iterator, batch, config)\n",
    "\n",
    "    # run training step\n",
    "    with nn_partitioning.axis_rules(config.logical_axis_rules):\n",
    "      state, metrics, nextrng = p_train_step(\n",
    "          model, config, state, batch, nextrng\n",
    "      )\n",
    "\n",
    "    learning_loss = metrics['scalar']['learning/loss']\n",
    "    print(\"train step\", step, \"loss:\", learning_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
