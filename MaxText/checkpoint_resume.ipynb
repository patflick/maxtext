{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e892f7-e326-4e62-baf9-25a63fc38c9b",
   "metadata": {},
   "source": [
    "# MaxText Lingvo fine-tuning\n",
    "\n",
    "Loads a pre-existing checkpoint of the converged LG dummy model (single\n",
    "attention layer), and then runs some additional training steps (fine-tuning).\n",
    "\n",
    "### TODO:\n",
    "need to create/import all of:\n",
    "- [x] model: load from checkpoint\n",
    "- [x] tx: optimizer\n",
    "- [x] config\n",
    "- [x] init_rng\n",
    "- [x] mesh\n",
    "- [x] checkpoint_manager\n",
    "- [ ] basic training function\n",
    "- [ ] short version of this NB that uses just config and calls `train.train_loop`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd639915-a6b8-4c14-a3e6-27ef0dd5935b",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ddd68f-8f01-4062-a0aa-67ecbdadcd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('run_name', '1xv4-8'),\n",
       "             ('load_parameters_path', ''),\n",
       "             ('load_from_other_directory',\n",
       "              'gs://mazumdera-test-bucket/maxtext/lg/11032023/1/1xv3-8/checkpoints/'),\n",
       "             ('load_from_other_directory_step', 50),\n",
       "             ('reuse_example_batch', 0),\n",
       "             ('metrics_file', ''),\n",
       "             ('gcs_metrics', False),\n",
       "             ('dtype', dtype(bfloat16)),\n",
       "             ('int8_training', False),\n",
       "             ('global_parameter_scale', 1),\n",
       "             ('base_emb_dim', 512),\n",
       "             ('base_num_heads', 4),\n",
       "             ('base_mlp_dim', 2048),\n",
       "             ('base_num_decoder_layers', 1),\n",
       "             ('head_dim', 96),\n",
       "             ('mlp_activations', ['gelu']),\n",
       "             ('dropout_rate', 0),\n",
       "             ('logits_via_embedding', True),\n",
       "             ('remat_policy', 'full'),\n",
       "             ('scan_layers', True),\n",
       "             ('param_scan_axis', 1),\n",
       "             ('enable_flash_attention', True),\n",
       "             ('record_internal_nn_metrics', 0),\n",
       "             ('base_output_directory',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/lingvo/20231108/1'),\n",
       "             ('mesh_axes', ['data', 'fsdp', 'tensor']),\n",
       "             ('logical_axis_rules',\n",
       "              (('activation_batch', ('data', 'fsdp')),\n",
       "               ('activation_length', ('data', 'fsdp')),\n",
       "               ('activation_embed', 'tensor'),\n",
       "               ('activation_mlp', 'tensor'),\n",
       "               ('activation_heads', 'tensor'),\n",
       "               ('activation_kv', 'tensor'),\n",
       "               ('activation_vocab', 'tensor'),\n",
       "               ('mlp', 'tensor'),\n",
       "               ('vocab', 'tensor'),\n",
       "               ('embed', 'fsdp'),\n",
       "               ('heads', 'tensor'))),\n",
       "             ('data_sharding', (('data', 'fsdp', 'tensor'),)),\n",
       "             ('dcn_data_parallelism', 1),\n",
       "             ('dcn_fsdp_parallelism', 1),\n",
       "             ('dcn_tensor_parallelism', 1),\n",
       "             ('ici_data_parallelism', 2),\n",
       "             ('ici_fsdp_parallelism', 1),\n",
       "             ('ici_tensor_parallelism', 2),\n",
       "             ('dataset_path', ''),\n",
       "             ('vocab_size', 50272),\n",
       "             ('assets_path', 'assets'),\n",
       "             ('vocab_relative_path', 'tokenizer'),\n",
       "             ('dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_dataset_name', 'c4/en:3.0.1'),\n",
       "             ('eval_split', 'validation'),\n",
       "             ('per_device_batch_size', 0.5),\n",
       "             ('eval_per_device_batch_size', 0),\n",
       "             ('max_corpus_chars', 10000000),\n",
       "             ('dataset_type', 'lg'),\n",
       "             ('file_pattern_for_train_data',\n",
       "              'gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords'),\n",
       "             ('file_pattern_for_eval_data',\n",
       "              'gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords'),\n",
       "             ('steps', 20),\n",
       "             ('log_period', 100),\n",
       "             ('save_period', 5),\n",
       "             ('learning_rate', 3e-05),\n",
       "             ('cosine_learning_rate_final_fraction', 0.1),\n",
       "             ('warmup_steps_fraction', 0.1),\n",
       "             ('learning_rate_schedule_steps', 20),\n",
       "             ('max_target_length', 2048),\n",
       "             ('max_eval_target_length', 512),\n",
       "             ('max_predict_length', 512),\n",
       "             ('sampling_temperature', 0.6),\n",
       "             ('sampling_top_k', 20),\n",
       "             ('eos_id', 2),\n",
       "             ('prompt', 'I love to '),\n",
       "             ('enable_profiler', True),\n",
       "             ('enable_checkpointing', True),\n",
       "             ('async_checkpointing', True),\n",
       "             ('enable_dropout', True),\n",
       "             ('enable_data_shuffling', True),\n",
       "             ('data_shuffle_seed', 0),\n",
       "             ('init_weights_seed', 0),\n",
       "             ('gradient_clipping_threshold', 1.0),\n",
       "             ('adam_b1', 0.9),\n",
       "             ('adam_b2', 0.95),\n",
       "             ('adam_eps', 1e-08),\n",
       "             ('adam_eps_root', 0.0),\n",
       "             ('adam_weight_decay', 0.1),\n",
       "             ('collect_stack_trace', False),\n",
       "             ('stack_trace_to_cloud', False),\n",
       "             ('stack_trace_interval_seconds', 600),\n",
       "             ('use_iota_embed', False),\n",
       "             ('compiled_trainstep_file', ''),\n",
       "             ('compile_topology', ''),\n",
       "             ('compile_topology_num_slices', -1),\n",
       "             ('tensorboard_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/lingvo/20231108/1/1xv4-8/tensorboard/'),\n",
       "             ('checkpoint_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/lingvo/20231108/1/1xv4-8/checkpoints/'),\n",
       "             ('metrics_dir',\n",
       "              'gs://patflick-maxtext-lingvo/maxtext/lingvo/20231108/1/1xv4-8/metrics/'),\n",
       "             ('emb_dim', 512),\n",
       "             ('num_heads', 4),\n",
       "             ('mlp_dim', 2048),\n",
       "             ('num_decoder_layers', 1),\n",
       "             ('global_batch_size_to_load', 4),\n",
       "             ('global_batch_size_to_train_on', 0)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#config \n",
    "my_bucket = \"gs://patflick-maxtext-lingvo\"\n",
    "base_output_directory=my_bucket + \"/maxtext/lingvo/20231108/1\"\n",
    "\n",
    "# Aisha's checkpoints\n",
    "#load_checkpoint_dir=\"gs://mazumdera-test-bucket/maxtext/lg/10142023/1/1xv3-8/checkpoints/\"\n",
    "load_checkpoint_dir=\"gs://mazumdera-test-bucket/maxtext/lg/11032023/1/1xv3-8/checkpoints/\"\n",
    "#base_output_directory=\"base_output_directory=gs://mazumdera-test-bucket/maxtext/lg/11032023/1\"\n",
    "\n",
    "# Train/Eval data\n",
    "file_pattern_for_train_data=\"file_pattern_for_train_data=gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords\"\n",
    "file_pattern_for_eval_data=\"file_pattern_for_eval_data=gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords\"\n",
    "\n",
    "base_num_decoder_layers=\"base_num_decoder_layers=1\"\n",
    "base_num_heads = \"base_num_heads=4\"\n",
    "head_nums = \"head_dim=96\"\n",
    "dataset_type = \"dataset_type=lg\"\n",
    "\n",
    "commandline_args = [\"dummy\", \n",
    "                    \"configs/base.yml\",\n",
    "                    \"run_name=1xv4-8\",\n",
    "                    \"dcn_data_parallelism=1\",\n",
    "                    \"save_period=5\",\n",
    "                    # TODO: configure parallelism!\n",
    "                    \"ici_data_parallelism=2\",\n",
    "                    \"ici_tensor_parallelism=2\",\n",
    "                    \"ici_fsdp_parallelism=1\",\n",
    "                    \"steps=20\",\n",
    "                    \"enable_profiler=true\",\n",
    "                    \"remat_policy=full\",\n",
    "                    \"base_emb_dim=512\", \n",
    "                    base_num_heads,\n",
    "                    head_nums,\n",
    "                    \"vocab_size=50272\",\n",
    "                    base_num_decoder_layers,\n",
    "                    \"per_device_batch_size=0.5\",\n",
    "                    \"enable_profiler=true\",\n",
    "                    \"base_mlp_dim=2048\", \n",
    "                    # File dependencies\n",
    "                    file_pattern_for_train_data, \n",
    "                    file_pattern_for_eval_data,\n",
    "                    \"base_output_directory=\" + base_output_directory,\n",
    "                    \"load_from_other_directory=\" + load_checkpoint_dir,\n",
    "                    \"load_from_other_directory_step=50\",\n",
    "                    dataset_type,\n",
    "                    \"max_predict_length=512\",\n",
    "                    #\"jax_default_prng_impl=unsafe_rgb\"   # required/overwritten by train.train_step. if not set here, will cause failures later\n",
    "                   ]\n",
    "\n",
    "import pyconfig\n",
    "pyconfig.initialize(commandline_args)\n",
    "config = pyconfig.config\n",
    "pyconfig._config.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d80d0-ca2f-4300-805e-ea2f7d2f50ca",
   "metadata": {},
   "source": [
    "## Loading pre-existing checkpoint\n",
    "\n",
    "Uses the config to create the optimizer, model, and mesh. Then loads the checkpoint into the model/optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3a3355-0983-4638-be83-77dae97130a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating checkpoint manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 22:32:36.269333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint manager created!\n"
     ]
    }
   ],
   "source": [
    "import checkpointing\n",
    "checkpoint_manager = checkpointing.create_orbax_checkpoint_manager(\n",
    "      checkpoint_dir = config.checkpoint_dir,\n",
    "      enable_checkpointing = True,   # need to be true to allow loading other checkpints\n",
    "      use_async = config.async_checkpointing,\n",
    "      save_interval_steps = config.save_period\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabc3c29-c24f-4d73-b1bd-deafac3b9b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f663ef66-3c24-4327-91ec-745a6576bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] (num_devices: 4)\n",
      "Decided on mesh: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)]]\n",
      "\n",
      " [[TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)\n",
      "   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mesh(device_ids=array([[[0, 1]],\n",
       "\n",
       "       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup device mesh\n",
    "import max_utils\n",
    "from jax.sharding import Mesh\n",
    "\n",
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = Mesh(devices_array, config.mesh_axes)\n",
    "mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff51a358-d6ab-4745-8b49-5a1b63cd5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "# Initial PRNG Keys\n",
    "jax.config.update('jax_default_prng_impl', 'unsafe_rbg')   # need to set here. train.py later sets this and then causes shape mismatch\n",
    "init_rng, nextrng = random.split(random.PRNGKey(config.init_weights_seed), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d047d60-1926-4148-aecd-cb1a9be2651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "from layers import Transformer\n",
    "model = Transformer(config, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1feb2085-fdc2-4aa9-a4e1-9c2f18ab36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "# create optimizer\n",
    "#TODO(from original notebook): also compare with optax.adafactor\n",
    "tx = optax.adamw(\n",
    "       max_utils.create_learning_rate_schedule(config),\n",
    "       b1=config.adam_b1,\n",
    "       b2=config.adam_b2,\n",
    "       eps=config.adam_eps,\n",
    "       eps_root=config.adam_eps_root,\n",
    "       weight_decay=config.adam_weight_decay,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0fe610-4ed1-461e-a020-9ffe7ff9eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring state from gs://mazumdera-test-bucket/maxtext/lg/11032023/1/1xv3-8/checkpoints/ step 50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dict key mismatch; expected keys: ['mlp', 'pre_mlp_layer_norm', 'pre_self_attention_layer_norm', 'relpos_bias', 'self_attention']; dict: {'mlp': {'wi': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None, 'tensor'), sharding=None, global_shape=None)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('tensor', None, 'fsdp'), sharding=None, global_shape=None)}}, 'pre_mlp_layer_norm': {'scale': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec(None, None), sharding=None, global_shape=None)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None), sharding=None, global_shape=None)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None, 'tensor', None), sharding=None, global_shape=None)}, 'key_layer_norm': {'scale': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('tensor', None), sharding=None, global_shape=None)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('tensor', None, None, 'fsdp'), sharding=None, global_shape=None)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None, 'tensor', None), sharding=None, global_shape=None)}, 'query_layer_norm': {'scale': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('tensor', None), sharding=None, global_shape=None)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None, 'tensor', None), sharding=None, global_shape=None)}}}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load checkpoint\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m state_read_from_ckpt, state_mesh_annotations_read_from_ckpt \u001b[39m=\u001b[39m max_utils\u001b[39m.\u001b[39;49msetup_initial_state(model, tx, config, init_rng, mesh, checkpoint_manager)\n",
      "File \u001b[0;32m~/dev/git/maxtext/MaxText/max_utils.py:205\u001b[0m, in \u001b[0;36msetup_initial_state\u001b[0;34m(model, tx, config, rng, mesh, checkpoint_manager)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m# Initialization\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mwith\u001b[39;00m nn_partitioning\u001b[39m.\u001b[39maxis_rules(config\u001b[39m.\u001b[39mlogical_axis_rules):\n\u001b[0;32m--> 205\u001b[0m   state, raw_params \u001b[39m=\u001b[39m checkpointing\u001b[39m.\u001b[39;49mload_state_if_possible(checkpoint_manager,\n\u001b[1;32m    206\u001b[0m                                               config\u001b[39m.\u001b[39;49mload_parameters_path,\n\u001b[1;32m    207\u001b[0m                                               config\u001b[39m.\u001b[39;49mload_from_other_directory,\n\u001b[1;32m    208\u001b[0m                                               config\u001b[39m.\u001b[39;49mload_from_other_directory_step,\n\u001b[1;32m    209\u001b[0m                                               unboxed_abstract_state,\n\u001b[1;32m    210\u001b[0m                                               mesh,\n\u001b[1;32m    211\u001b[0m                                               state_mesh_annotations)\n\u001b[1;32m    213\u001b[0m   state_mesh_shardings \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_map(\n\u001b[1;32m    214\u001b[0m       \u001b[39mlambda\u001b[39;00m p: jax\u001b[39m.\u001b[39msharding\u001b[39m.\u001b[39mNamedSharding(mesh, p), state_mesh_annotations)\n\u001b[1;32m    215\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m state:\n",
      "File \u001b[0;32m~/dev/git/maxtext/MaxText/checkpointing.py:147\u001b[0m, in \u001b[0;36mload_state_if_possible\u001b[0;34m(checkpoint_manager, first_checkpoint_path, load_from_other_directory, load_from_other_directory_step, abstract_unboxed_pre_state, mesh, state_mesh_annotations)\u001b[0m\n\u001b[1;32m    145\u001b[0m     step \u001b[39m=\u001b[39m load_from_other_directory_step\n\u001b[1;32m    146\u001b[0m     max_logging\u001b[39m.\u001b[39mlog(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrestoring state from \u001b[39m\u001b[39m{\u001b[39;00mload_from_other_directory\u001b[39m}\u001b[39;00m\u001b[39m step \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mreturn\u001b[39;00m mngr_loader\u001b[39m.\u001b[39;49mrestore(step, abstract_unboxed_pre_state,\n\u001b[1;32m    148\u001b[0m                                     {\u001b[39m\"\u001b[39;49m\u001b[39mrestore_args\u001b[39;49m\u001b[39m\"\u001b[39;49m : restore_args}), \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m   max_logging\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mNo existing checkpoints found, not restoring checkpoint.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/orbax/checkpoint/checkpoint_manager.py:618\u001b[0m, in \u001b[0;36mCheckpointManager.restore\u001b[0;34m(self, step, items, restore_kwargs, directory)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_single_item:\n\u001b[1;32m    616\u001b[0m   restore_kwargs \u001b[39m=\u001b[39m {DEFAULT_ITEM_NAME: restore_kwargs}\n\u001b[0;32m--> 618\u001b[0m restored_items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_restore_impl(\n\u001b[1;32m    619\u001b[0m     step, items, restore_kwargs, directory\u001b[39m=\u001b[39;49mdirectory)\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_single_item:\n\u001b[1;32m    622\u001b[0m   \u001b[39mreturn\u001b[39;00m restored_items[DEFAULT_ITEM_NAME]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/orbax/checkpoint/checkpoint_manager.py:650\u001b[0m, in \u001b[0;36mCheckpointManager._restore_impl\u001b[0;34m(self, step, items, restore_kwargs, directory)\u001b[0m\n\u001b[1;32m    648\u001b[0m   item \u001b[39m=\u001b[39m items\u001b[39m.\u001b[39mget(item_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    649\u001b[0m   kwargs \u001b[39m=\u001b[39m restore_kwargs\u001b[39m.\u001b[39mget(item_name, {})\n\u001b[0;32m--> 650\u001b[0m   restored[item_name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checkpointers[item_name]\u001b[39m.\u001b[39;49mrestore(\n\u001b[1;32m    651\u001b[0m       path, item\u001b[39m=\u001b[39;49mitem, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    653\u001b[0m \u001b[39mreturn\u001b[39;00m restored\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/orbax/checkpoint/checkpointer.py:99\u001b[0m, in \u001b[0;36mCheckpointer.restore\u001b[0;34m(self, directory, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFound incomplete checkpoint at \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mRestoring item from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, directory)\n\u001b[0;32m---> 99\u001b[0m restored \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handler\u001b[39m.\u001b[39;49mrestore(directory, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    100\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mFinished restoring checkpoint from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, directory)\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m restored\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:1052\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler.restore\u001b[0;34m(self, directory, item, restore_args, transforms, transforms_default_to_original, legacy_transform_fn)\u001b[0m\n\u001b[1;32m   1048\u001b[0m   \u001b[39mreturn\u001b[39;00m meta\n\u001b[1;32m   1050\u001b[0m \u001b[39m# If metadata file was missing in the checkpoint, we need to decide\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[39m# restore_type based on RestoreArgs.\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m structure \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mtree_util\u001b[39m.\u001b[39;49mtree_map(\n\u001b[1;32m   1053\u001b[0m     _maybe_set_default_restore_types, structure, checkpoint_restore_args\n\u001b[1;32m   1054\u001b[0m )\n\u001b[1;32m   1056\u001b[0m restored_item \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun(\n\u001b[1;32m   1057\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_deserialize(structure, param_infos, checkpoint_restore_args)\n\u001b[1;32m   1058\u001b[0m )\n\u001b[1;32m   1060\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m legacy_transform_fn:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/tree_util.py:243\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m  [[5, 7, 9], [6, 1, 2]]\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[39m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m--> 243\u001b[0m all_leaves \u001b[39m=\u001b[39m [leaves] \u001b[39m+\u001b[39m [treedef\u001b[39m.\u001b[39mflatten_up_to(r) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m rest]\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m treedef\u001b[39m.\u001b[39munflatten(f(\u001b[39m*\u001b[39mxs) \u001b[39mfor\u001b[39;00m xs \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/tree_util.py:243\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m  [[5, 7, 9], [6, 1, 2]]\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[39m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m--> 243\u001b[0m all_leaves \u001b[39m=\u001b[39m [leaves] \u001b[39m+\u001b[39m [treedef\u001b[39m.\u001b[39;49mflatten_up_to(r) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m rest]\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m treedef\u001b[39m.\u001b[39munflatten(f(\u001b[39m*\u001b[39mxs) \u001b[39mfor\u001b[39;00m xs \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mall_leaves))\n",
      "\u001b[0;31mValueError\u001b[0m: Dict key mismatch; expected keys: ['mlp', 'pre_mlp_layer_norm', 'pre_self_attention_layer_norm', 'relpos_bias', 'self_attention']; dict: {'mlp': {'wi': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None, 'tensor'), sharding=None, global_shape=None)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('tensor', None, 'fsdp'), sharding=None, global_shape=None)}}, 'pre_mlp_layer_norm': {'scale': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec(None, None), sharding=None, global_shape=None)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None), sharding=None, global_shape=None)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None, 'tensor', None), sharding=None, global_shape=None)}, 'key_layer_norm': {'scale': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('tensor', None), sharding=None, global_shape=None)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('tensor', None, None, 'fsdp'), sharding=None, global_shape=None)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None, 'tensor', None), sharding=None, global_shape=None)}, 'query_layer_norm': {'scale': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('tensor', None), sharding=None, global_shape=None)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=None, dtype=None, mesh=Mesh(device_ids=array([[[0, 1]],\n\n       [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor')), mesh_axes=PartitionSpec('fsdp', None, 'tensor', None), sharding=None, global_shape=None)}}}."
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "state_read_from_ckpt, state_mesh_annotations_read_from_ckpt = max_utils.setup_initial_state(model, tx, config, init_rng, mesh, checkpoint_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70149528-8ed6-426a-9455-c35b846abd3c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Print out the loaded transformer model architecture. This currently is a simple fake model with a single self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7434bae2-58d2-431b-b0b9-bfed48eb0e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder:\n",
      "  decoder:\n",
      "    mlp:\n",
      "      wi:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1, 2048)\n",
      "      wo:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(2048, 1, 512)\n",
      "    pre_mlp_layer_norm:\n",
      "      scale: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1)\n",
      "    pre_self_attention_layer_norm:\n",
      "      scale: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1)\n",
      "    relpos_bias:\n",
      "      rel_embedding: <class 'jaxlib.xla_extension.ArrayImpl'>(4, 1, 32)\n",
      "    self_attention:\n",
      "      key:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1, 4, 96)\n",
      "      out:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(4, 1, 96, 512)\n",
      "      query:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1, 4, 96)\n",
      "      value:\n",
      "        kernel: <class 'jaxlib.xla_extension.ArrayImpl'>(512, 1, 4, 96)\n",
      "  decoder_norm:\n",
      "    scale: <class 'jaxlib.xla_extension.ArrayImpl'>(512,)\n",
      "token_embedder:\n",
      "  embedding: <class 'jaxlib.xla_extension.ArrayImpl'>(50272, 512)\n"
     ]
    }
   ],
   "source": [
    "state = state_read_from_ckpt\n",
    "type(state.params)\n",
    "def print_params_shape(params):\n",
    "    def _print_params_dict_rec(sub_dict, ident=\"\"):\n",
    "        for key, value in sub_dict.items():\n",
    "            line = key\n",
    "            if type(key) != str:\n",
    "                line = str(type(key))\n",
    "            if type(value) == dict:\n",
    "                print(ident + line + \":\")\n",
    "                _print_params_dict_rec(value, ident + \"  \")\n",
    "            elif \"shape\" in dir(value):\n",
    "                print(ident + line + \": \" + str(type(value)) + str(value.shape))\n",
    "            else:\n",
    "                print(ident + line + \": \" + str(type(value)))\n",
    "    _print_params_dict_rec(params)\n",
    "            \n",
    "print_params_shape(state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a20b9-3a67-4d92-af15-d1024e58d3d4",
   "metadata": {},
   "source": [
    "### Create sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a22560-9328-4dc7-a19c-d69efbcae247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec as P\n",
    "\n",
    "# Compute sharding by combining checkpoint PartitionSpecs with config's mesh\n",
    "data_pspec = P(*config.data_sharding)\n",
    "state_mesh_shardings_read_from_ckpt = jax.tree_map(\n",
    "  lambda p: jax.sharding.NamedSharding(mesh, p), state_mesh_annotations_read_from_ckpt)\n",
    "data_sharding = jax.tree_map(\n",
    "  lambda p: jax.sharding.NamedSharding(mesh, p), data_pspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7cfcca-befc-44bd-9af6-8992bd7a49d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec(), memory_kind=tpu_hbm), apply_fn=<bound method Module.apply of Transformer(\n",
       "    # attributes\n",
       "    config = <pyconfig.HyperParameters object at 0x7f646e68f6a0>\n",
       "    mesh = Mesh(device_ids=array([[[0, 1]],\n",
       "    \n",
       "           [[2, 3]]]), axis_names=('data', 'fsdp', 'tensor'))\n",
       ")>, params={'decoder': {'decoder': {'mlp': {'wi': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor'), memory_kind=tpu_hbm)}, 'wo': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, 'fsdp'), memory_kind=tpu_hbm)}}, 'pre_mlp_layer_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec(None, None), memory_kind=tpu_hbm)}, 'pre_self_attention_layer_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None), memory_kind=tpu_hbm)}, 'relpos_bias': {'rel_embedding': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, None), memory_kind=tpu_hbm)}, 'self_attention': {'key': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}, 'out': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, None, 'fsdp'), memory_kind=tpu_hbm)}, 'query': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}, 'value': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}}}, 'decoder_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp',), memory_kind=tpu_hbm)}}, 'token_embedder': {'embedding': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', 'fsdp'), memory_kind=tpu_hbm)}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7f516ae0a9e0>, update=<function chain.<locals>.update_fn at 0x7f516ae0ac20>), opt_state=(ScaleByAdamState(count=NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec(), memory_kind=tpu_hbm), mu={'decoder': {'decoder': {'mlp': {'wi': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor'), memory_kind=tpu_hbm)}, 'wo': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, 'fsdp'), memory_kind=tpu_hbm)}}, 'pre_mlp_layer_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec(None, None), memory_kind=tpu_hbm)}, 'pre_self_attention_layer_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None), memory_kind=tpu_hbm)}, 'relpos_bias': {'rel_embedding': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, None), memory_kind=tpu_hbm)}, 'self_attention': {'key': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}, 'out': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, None, 'fsdp'), memory_kind=tpu_hbm)}, 'query': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}, 'value': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}}}, 'decoder_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp',), memory_kind=tpu_hbm)}}, 'token_embedder': {'embedding': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', 'fsdp'), memory_kind=tpu_hbm)}}, nu={'decoder': {'decoder': {'mlp': {'wi': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor'), memory_kind=tpu_hbm)}, 'wo': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, 'fsdp'), memory_kind=tpu_hbm)}}, 'pre_mlp_layer_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec(None, None), memory_kind=tpu_hbm)}, 'pre_self_attention_layer_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None), memory_kind=tpu_hbm)}, 'relpos_bias': {'rel_embedding': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, None), memory_kind=tpu_hbm)}, 'self_attention': {'key': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}, 'out': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', None, None, 'fsdp'), memory_kind=tpu_hbm)}, 'query': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}, 'value': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp', None, 'tensor', None), memory_kind=tpu_hbm)}}}, 'decoder_norm': {'scale': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('fsdp',), memory_kind=tpu_hbm)}}, 'token_embedder': {'embedding': NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec('tensor', 'fsdp'), memory_kind=tpu_hbm)}}), EmptyState(), ScaleByScheduleState(count=NamedSharding(mesh=Mesh('data': 2, 'fsdp': 1, 'tensor': 2), spec=PartitionSpec(), memory_kind=tpu_hbm))))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mesh_shardings_read_from_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7cc7f-c8c4-4027-b520-f776868b7b7a",
   "metadata": {},
   "source": [
    "## Training (fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f9ee03-7bec-4bd4-9534-8dbd4247d344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patflick/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read training data from path: gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords\n",
      "Training dataset has: 500000 entries\n",
      "Trying to read eval data from path: gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords\n",
      "Eval dataset has: 50000 entries\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "from input_pipeline import create_data_iterator_with_tokenizer\n",
    "\n",
    "data_iterator = create_data_iterator_with_tokenizer(config, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95826d5c-10f2-4a3a-86aa-76671df1d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.compilation_cache:Initialized persistent compilation cache at /home/patflick/jax_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step 0 loss: 14.631306\n",
      "train step 1 loss: 14.562592\n",
      "train step 2 loss: 14.633102\n",
      "train step 3 loss: 14.545918\n",
      "train step 4 loss: 14.475461\n",
      "train step 5 loss: 14.462444\n",
      "train step 6 loss: 14.40369\n",
      "train step 7 loss: 14.342745\n",
      "train step 8 loss: 14.338179\n",
      "train step 9 loss: 14.263444\n",
      "train step 10 loss: 14.306589\n",
      "train step 11 loss: 14.147816\n",
      "train step 12 loss: 14.281685\n",
      "train step 13 loss: 14.175036\n",
      "train step 14 loss: 14.167313\n",
      "train step 15 loss: 14.149162\n",
      "train step 16 loss: 14.129879\n",
      "train step 17 loss: 14.160196\n",
      "train step 18 loss: 14.130515\n",
      "train step 19 loss: 14.150421\n",
      "train step 20 loss: 14.140012\n",
      "train step 21 loss: 14.102599\n",
      "train step 22 loss: 14.110823\n",
      "train step 23 loss: 14.056253\n",
      "train step 24 loss: 14.037472\n",
      "train step 25 loss: 14.111221\n",
      "train step 26 loss: 14.026969\n",
      "train step 27 loss: 14.084249\n",
      "train step 28 loss: 14.057688\n",
      "train step 29 loss: 14.013303\n",
      "train step 30 loss: 14.003209\n",
      "train step 31 loss: 13.990495\n",
      "train step 32 loss: 13.924233\n",
      "train step 33 loss: 13.874407\n",
      "train step 34 loss: 13.807951\n",
      "train step 35 loss: 13.7564\n",
      "train step 36 loss: 13.720367\n",
      "train step 37 loss: 13.665394\n",
      "train step 38 loss: 13.623045\n",
      "train step 39 loss: 13.546552\n",
      "train step 40 loss: 13.560575\n",
      "train step 41 loss: 13.405075\n",
      "train step 42 loss: 13.4779005\n",
      "train step 43 loss: 13.364978\n",
      "train step 44 loss: 13.287406\n",
      "train step 45 loss: 13.237704\n",
      "train step 46 loss: 13.275438\n",
      "train step 47 loss: 13.24605\n",
      "train step 48 loss: 13.137706\n",
      "train step 49 loss: 13.154662\n",
      "train step 50 loss: 13.154743\n",
      "train step 51 loss: 13.193561\n",
      "train step 52 loss: 13.076759\n",
      "train step 53 loss: 13.097267\n",
      "train step 54 loss: 13.10605\n",
      "train step 55 loss: 13.040205\n",
      "train step 56 loss: 13.069339\n",
      "train step 57 loss: 13.099377\n",
      "train step 58 loss: 13.066286\n",
      "train step 59 loss: 13.054611\n",
      "train step 60 loss: 13.062199\n",
      "train step 61 loss: 13.037296\n",
      "train step 62 loss: 13.046855\n",
      "train step 63 loss: 13.055891\n",
      "train step 64 loss: 12.95364\n",
      "train step 65 loss: 12.882456\n",
      "train step 66 loss: 12.932061\n",
      "train step 67 loss: 12.92765\n",
      "train step 68 loss: 12.824696\n",
      "train step 69 loss: 12.8050995\n",
      "train step 70 loss: 12.758886\n",
      "train step 71 loss: 12.739304\n",
      "train step 72 loss: 12.696633\n",
      "train step 73 loss: 12.684133\n",
      "train step 74 loss: 12.636344\n",
      "train step 75 loss: 12.510844\n",
      "train step 76 loss: 12.500152\n",
      "train step 77 loss: 12.5017805\n",
      "train step 78 loss: 12.482123\n",
      "train step 79 loss: 12.42386\n",
      "train step 80 loss: 12.362133\n",
      "train step 81 loss: 12.327837\n",
      "train step 82 loss: 12.274485\n",
      "train step 83 loss: 12.2591095\n",
      "train step 84 loss: 12.271456\n",
      "train step 85 loss: 12.218296\n",
      "train step 86 loss: 12.239998\n",
      "train step 87 loss: 12.138154\n",
      "train step 88 loss: 12.17888\n",
      "train step 89 loss: 12.159246\n",
      "train step 90 loss: 12.19495\n",
      "train step 91 loss: 12.172428\n",
      "train step 92 loss: 12.195517\n",
      "train step 93 loss: 12.145956\n",
      "train step 94 loss: 12.171661\n",
      "train step 95 loss: 12.119516\n",
      "train step 96 loss: 12.148457\n",
      "train step 97 loss: 12.136761\n",
      "train step 98 loss: 12.153879\n",
      "train step 99 loss: 12.104862\n",
      "train step 100 loss: 12.142239\n",
      "train step 101 loss: 12.05998\n",
      "train step 102 loss: 12.056662\n",
      "train step 103 loss: 12.087397\n",
      "train step 104 loss: 12.054815\n",
      "train step 105 loss: 11.970721\n",
      "train step 106 loss: 11.969517\n",
      "train step 107 loss: 11.903563\n",
      "train step 108 loss: 11.914379\n",
      "train step 109 loss: 11.933643\n",
      "train step 110 loss: 11.846758\n",
      "train step 111 loss: 11.847926\n",
      "train step 112 loss: 11.817692\n",
      "train step 113 loss: 11.772938\n",
      "train step 114 loss: 11.753301\n",
      "train step 115 loss: 11.746452\n",
      "train step 116 loss: 11.732599\n",
      "train step 117 loss: 11.701145\n",
      "train step 118 loss: 11.679696\n",
      "train step 119 loss: 11.681889\n",
      "train step 120 loss: 11.692308\n",
      "train step 121 loss: 11.641618\n",
      "train step 122 loss: 11.624442\n",
      "train step 123 loss: 11.661229\n",
      "train step 124 loss: 11.655899\n",
      "train step 125 loss: 11.651205\n",
      "train step 126 loss: 11.626257\n",
      "train step 127 loss: 11.621693\n",
      "train step 128 loss: 11.643497\n",
      "train step 129 loss: 11.64618\n",
      "train step 130 loss: 11.610243\n",
      "train step 131 loss: 11.63113\n",
      "train step 132 loss: 11.647692\n",
      "train step 133 loss: 11.6168995\n",
      "train step 134 loss: 11.617231\n",
      "train step 135 loss: 11.626816\n",
      "train step 136 loss: 11.636816\n",
      "train step 137 loss: 11.623615\n",
      "train step 138 loss: 11.6333885\n",
      "train step 139 loss: 11.59185\n",
      "train step 140 loss: 11.578285\n",
      "train step 141 loss: 11.594561\n",
      "train step 142 loss: 11.598542\n",
      "train step 143 loss: 11.589252\n",
      "train step 144 loss: 11.521009\n",
      "train step 145 loss: 11.541799\n",
      "train step 146 loss: 11.494761\n",
      "train step 147 loss: 11.534661\n",
      "train step 148 loss: 11.495004\n",
      "train step 149 loss: 11.494776\n",
      "train step 150 loss: 11.496066\n",
      "train step 151 loss: 11.487065\n",
      "train step 152 loss: 11.469088\n",
      "train step 153 loss: 11.503151\n",
      "train step 154 loss: 11.472719\n",
      "train step 155 loss: 11.475638\n",
      "train step 156 loss: 11.462727\n",
      "train step 157 loss: 11.472843\n",
      "train step 158 loss: 11.450966\n",
      "train step 159 loss: 11.44786\n",
      "train step 160 loss: 11.46073\n",
      "train step 161 loss: 11.4642\n",
      "train step 162 loss: 11.465418\n",
      "train step 163 loss: 11.464313\n",
      "train step 164 loss: 11.462107\n",
      "train step 165 loss: 11.463078\n",
      "train step 166 loss: 11.462033\n",
      "train step 167 loss: 11.462391\n",
      "train step 168 loss: 11.436304\n",
      "train step 169 loss: 11.434465\n",
      "train step 170 loss: 11.443695\n",
      "train step 171 loss: 11.469553\n",
      "train step 172 loss: 11.429011\n",
      "train step 173 loss: 11.458765\n",
      "train step 174 loss: 11.470577\n",
      "train step 175 loss: 11.452942\n",
      "train step 176 loss: 11.422686\n",
      "train step 177 loss: 11.43799\n",
      "train step 178 loss: 11.410048\n",
      "train step 179 loss: 11.426436\n",
      "train step 180 loss: 11.410971\n",
      "train step 181 loss: 11.428827\n",
      "train step 182 loss: 11.391268\n",
      "train step 183 loss: 11.398695\n",
      "train step 184 loss: 11.417648\n",
      "train step 185 loss: 11.413223\n",
      "train step 186 loss: 11.414954\n",
      "train step 187 loss: 11.418715\n",
      "train step 188 loss: 11.40492\n",
      "train step 189 loss: 11.391626\n",
      "train step 190 loss: 11.378009\n",
      "train step 191 loss: 11.406996\n",
      "train step 192 loss: 11.372505\n",
      "train step 193 loss: 11.420237\n",
      "train step 194 loss: 11.398385\n",
      "train step 195 loss: 11.393089\n",
      "train step 196 loss: 11.394821\n",
      "train step 197 loss: 11.362976\n",
      "train step 198 loss: 11.374581\n",
      "train step 199 loss: 11.424252\n",
      "train step 200 loss: 11.375099\n",
      "train step 201 loss: 11.414865\n",
      "train step 202 loss: 11.378299\n",
      "train step 203 loss: 11.390569\n",
      "train step 204 loss: 11.365252\n",
      "train step 205 loss: 11.389817\n",
      "train step 206 loss: 11.3958\n",
      "train step 207 loss: 11.40832\n",
      "train step 208 loss: 11.38829\n",
      "train step 209 loss: 11.364759\n",
      "train step 210 loss: 11.401096\n",
      "train step 211 loss: 11.37233\n",
      "train step 212 loss: 11.385322\n",
      "train step 213 loss: 11.373508\n",
      "train step 214 loss: 11.358547\n",
      "train step 215 loss: 11.383841\n",
      "train step 216 loss: 11.384816\n",
      "train step 217 loss: 11.3525505\n",
      "train step 218 loss: 11.372101\n",
      "train step 219 loss: 11.3805275\n",
      "train step 220 loss: 11.367212\n",
      "train step 221 loss: 11.365607\n",
      "train step 222 loss: 11.346829\n",
      "train step 223 loss: 11.360383\n",
      "train step 224 loss: 11.363075\n",
      "train step 225 loss: 11.3722725\n",
      "train step 226 loss: 11.359803\n",
      "train step 227 loss: 11.4000025\n",
      "train step 228 loss: 11.341426\n",
      "train step 229 loss: 11.360929\n",
      "train step 230 loss: 11.390487\n",
      "train step 231 loss: 11.373981\n",
      "train step 232 loss: 11.36127\n",
      "train step 233 loss: 11.334916\n",
      "train step 234 loss: 11.391111\n",
      "train step 235 loss: 11.385879\n",
      "train step 236 loss: 11.362537\n",
      "train step 237 loss: 11.35046\n",
      "train step 238 loss: 11.396226\n",
      "train step 239 loss: 11.358519\n",
      "train step 240 loss: 11.364183\n",
      "train step 241 loss: 11.352212\n",
      "train step 242 loss: 11.382589\n",
      "train step 243 loss: 11.346326\n",
      "train step 244 loss: 11.364679\n",
      "train step 245 loss: 11.36391\n",
      "train step 246 loss: 11.363294\n",
      "train step 247 loss: 11.333699\n",
      "train step 248 loss: 11.385641\n",
      "train step 249 loss: 11.3701935\n",
      "train step 250 loss: 11.384686\n",
      "train step 251 loss: 11.344757\n",
      "train step 252 loss: 11.323548\n",
      "train step 253 loss: 11.353901\n",
      "train step 254 loss: 11.36951\n",
      "train step 255 loss: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m   state, metrics, nextrng \u001b[39m=\u001b[39m p_train_step(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m       model, config, state, batch, nextrng\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m   )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m learning_loss \u001b[39m=\u001b[39m metrics[\u001b[39m'\u001b[39m\u001b[39mscalar\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlearning/loss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mtrain step\u001b[39;49m\u001b[39m\"\u001b[39;49m, step, \u001b[39m\"\u001b[39;49m\u001b[39mloss:\u001b[39;49m\u001b[39m\"\u001b[39;49m, learning_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# TODO: collect and write out metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcloud-dev/home/patflick/dev/git/maxtext/MaxText/checkpoint_resume.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# TODO: checkpointing\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/array.py:250\u001b[0m, in \u001b[0;36mArrayImpl.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 250\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_value)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/profiler.py:340\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    339\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    341\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/array.py:566\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_npy_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_npy_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_single_device_array_to_np_array()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    567\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_npy_value\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(np\u001b[39m.\u001b[39mndarray, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_npy_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import train\n",
    "import numpy as np\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "\n",
    "# compile train function\n",
    "p_train_step = jax.jit(train.train_step,\n",
    "                       in_shardings=(state_mesh_shardings_read_from_ckpt, data_sharding, None),\n",
    "                       out_shardings=(state_mesh_shardings_read_from_ckpt, None, None),\n",
    "                       static_argnums=(0,1,),\n",
    "                       donate_argnums=2)\n",
    "\n",
    "# run fine-tuning training\n",
    "batch = None\n",
    "for step in np.arange(train.get_first_step(state), 1000):\n",
    "    # load batch\n",
    "    batch = train.load_next_batch(data_iterator, batch, config)\n",
    "\n",
    "    # run training step\n",
    "    with nn_partitioning.axis_rules(config.logical_axis_rules):\n",
    "      state, metrics, nextrng = p_train_step(\n",
    "          model, config, state, batch, nextrng\n",
    "      )\n",
    "\n",
    "    learning_loss = metrics['scalar']['learning/loss']\n",
    "    print(\"train step\", step, \"loss:\", learning_loss)\n",
    "    # TODO: collect and write out metrics\n",
    "    # TODO: checkpointing\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
